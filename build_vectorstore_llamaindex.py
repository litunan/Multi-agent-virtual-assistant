#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ LlamaIndex æ„å»º FAISS å‘é‡æ•°æ®åº“

ä¸ LangChain ç‰ˆæœ¬çš„å¯¹æ¯”ï¼š
- LangChain: DirectoryLoader + RecursiveCharacterTextSplitter
- LlamaIndex: SimpleDirectoryReader + SentenceSplitter

LlamaIndex ç‰¹ç‚¹ï¼š
1. æ›´çµæ´»çš„èŠ‚ç‚¹è§£æå™¨ï¼ˆNode Parsersï¼‰
2. å†…ç½®çš„æ–‡æ¡£æ‘˜è¦å’Œå…ƒæ•°æ®æå–
3. æ”¯æŒå±‚æ¬¡åŒ–ç´¢å¼•ç»“æ„
4. æ›´å¥½çš„ä¸­æ–‡åˆ†å¥æ”¯æŒ
"""

import os
from pathlib import Path


def build_vectorstore_llamaindex():
    """ä½¿ç”¨ LlamaIndex ä» RAG_Document æ–‡ä»¶å¤¹æ„å»ºå‘é‡æ•°æ®åº“"""
    
    # å»¶è¿Ÿå¯¼å…¥ï¼Œä¾¿äºæ£€æŸ¥ä¾èµ–
    from llama_index.core import (
        SimpleDirectoryReader,
        VectorStoreIndex,
        StorageContext,
        Settings,
    )
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.vector_stores.faiss import FaissVectorStore
    import faiss
    
    print("=" * 60)
    print("ğŸ¦™ ä½¿ç”¨ LlamaIndex æ„å»ºå‘é‡æ•°æ®åº“")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹
    print("\nğŸ“¦ æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹...")
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        device="cpu",
        normalize=True,
    )
    print("âœ… Embedding æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 2. é…ç½®å…¨å±€è®¾ç½®
    Settings.embed_model = embed_model
    Settings.chunk_size = 1000
    Settings.chunk_overlap = 200
    
    # 3. åŠ è½½æ–‡æ¡£
    print("\nğŸ“„ æ­£åœ¨åŠ è½½æ–‡æ¡£...")
    documents = SimpleDirectoryReader(
        input_dir="RAG_Document",
        recursive=True,  # é€’å½’è¯»å–å­ç›®å½•
        required_exts=[".md"],  # åªè¯»å– markdown æ–‡ä»¶
        filename_as_id=True,  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ–‡æ¡£ID
    ).load_data()
    print(f"âœ… å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºåŠ è½½çš„æ–‡æ¡£ä¿¡æ¯
    for doc in documents:
        print(f"   ğŸ“„ {doc.metadata.get('file_name', 'unknown')}")
    
    # 4. é…ç½®æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆNode Parserï¼‰
    print("\nâœ‚ï¸ æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
    node_parser = SentenceSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separator=" ",  # ä¸»è¦åˆ†éš”ç¬¦
        paragraph_separator="\n\n",  # æ®µè½åˆ†éš”ç¬¦
        secondary_chunking_regex="[ã€‚ï¼ï¼Ÿï¼›\n]",  # ä¸­æ–‡å¥å­åˆ†éš”
    )
    
    # è§£ææ–‡æ¡£ä¸ºèŠ‚ç‚¹
    nodes = node_parser.get_nodes_from_documents(documents)
    print(f"âœ… æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹")
    
    # 5. åˆ›å»º FAISS å‘é‡å­˜å‚¨
    print("\nğŸ”§ æ­£åœ¨æ„å»º FAISS å‘é‡ç´¢å¼•...")
    
    # è·å– embedding ç»´åº¦
    sample_embedding = embed_model.get_text_embedding("test")
    embedding_dim = len(sample_embedding)
    print(f"   Embedding ç»´åº¦: {embedding_dim}")
    
    # åˆ›å»º FAISS ç´¢å¼•
    faiss_index = faiss.IndexFlatL2(embedding_dim)
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = FaissVectorStore(faiss_index=faiss_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # 6. æ„å»ºç´¢å¼•
    print("\nğŸ—ï¸ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex(
        nodes=nodes,
        storage_context=storage_context,
        embed_model=embed_model,
        show_progress=True,
    )
    
    # 7. ä¿å­˜ç´¢å¼•
    output_dir = "mcp_course_materials_db_llamaindex"
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•åˆ° {output_dir}/...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜åˆ°ç£ç›˜
    index.storage_context.persist(persist_dir=output_dir)
    
    print("\n" + "=" * 60)
    print("âœ… LlamaIndex å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"   - èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
    print(f"   - Embedding ç»´åº¦: {embedding_dim}")
    print(f"   - å­˜å‚¨ä½ç½®: {output_dir}/")
    
    return index


def test_query(index=None):
    """æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½"""
    from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.vector_stores.faiss import FaissVectorStore
    from llama_index.core import Settings
    from llama_index.llms.openai_like import OpenAILike
    import faiss
    import sys
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from config.load_key import load_key
    
    output_dir = "mcp_course_materials_db_llamaindex"
    
    # åˆå§‹åŒ– LLM - ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ API
    print("\nğŸ¤– æ­£åœ¨åˆå§‹åŒ– LLM...")
    llm = OpenAILike(
        api_key=load_key("aliyun-bailian"),
        api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model="qwen-plus",
        is_chat_model=True,
    )
    Settings.llm = llm
    print("âœ… LLM åˆå§‹åŒ–æˆåŠŸ (qwen-plus)")
    
    if index is None:
        print("\nğŸ” æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•...")
        
        # é‡æ–°åˆå§‹åŒ– embedding æ¨¡å‹
        embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            device="cpu",
            normalize=True,
        )
        Settings.embed_model = embed_model
        
        # åŠ è½½ FAISS ç´¢å¼•
        vector_store = FaissVectorStore.from_persist_dir(output_dir)
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
            persist_dir=output_dir,
        )
        
        index = load_index_from_storage(storage_context, embed_model=embed_model)
        print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=3,  # è¿”å›å‰3ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
        llm=llm,
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "æŠ—ç™Œè‚½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æŠ—ç™Œè‚½çš„ä½œç”¨æœºåˆ¶æœ‰å“ªäº›ï¼Ÿ",
    ]
    
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•æŸ¥è¯¢")
    print("=" * 60)
    
    for query in test_queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        print("-" * 40)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        response = query_engine.query(query)
        
        print(f"ğŸ“ å›ç­”: {response.response[:500]}...")
        
        # æ˜¾ç¤ºæ¥æº
        if response.source_nodes:
            print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(response.source_nodes)} ä¸ª):")
            for i, node in enumerate(response.source_nodes, 1):
                score = node.score if hasattr(node, 'score') else 'N/A'
                file_name = node.metadata.get('file_name', 'æœªçŸ¥')
                print(f"   {i}. {file_name} (ç›¸å…³åº¦: {score:.4f})")


def compare_with_langchain():
    """å¯¹æ¯” LangChain å’Œ LlamaIndex çš„åˆ†å—ç»“æœ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š LangChain vs LlamaIndex å¯¹æ¯”")
    print("=" * 60)
    
    # è¯»å–ä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£
    sample_file = "RAG_Document/001/001_updated.md"
    if not os.path.exists(sample_file):
        print(f"âŒ ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {sample_file}")
        return
    
    with open(sample_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"\nğŸ“„ ç¤ºä¾‹æ–‡æ¡£: {sample_file}")
    print(f"   æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # LangChain åˆ†å—
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        langchain_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )
        langchain_chunks = langchain_splitter.split_text(content)
        print(f"\nğŸ”— LangChain åˆ†å—ç»“æœ: {len(langchain_chunks)} å—")
        print(f"   å¹³å‡å—å¤§å°: {sum(len(c) for c in langchain_chunks) / len(langchain_chunks):.0f} å­—ç¬¦")
        
    except ImportError:
        print("\nâš ï¸ LangChain æœªå®‰è£…ï¼Œè·³è¿‡å¯¹æ¯”")
        langchain_chunks = []
    
    # LlamaIndex åˆ†å—
    try:
        from llama_index.core.node_parser import SentenceSplitter
        from llama_index.core import Document
        
        llamaindex_splitter = SentenceSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            secondary_chunking_regex="[ã€‚ï¼ï¼Ÿï¼›\n]",
        )
        
        doc = Document(text=content)
        llamaindex_nodes = llamaindex_splitter.get_nodes_from_documents([doc])
        llamaindex_chunks = [node.get_content() for node in llamaindex_nodes]
        
        print(f"\nğŸ¦™ LlamaIndex åˆ†å—ç»“æœ: {len(llamaindex_chunks)} å—")
        print(f"   å¹³å‡å—å¤§å°: {sum(len(c) for c in llamaindex_chunks) / len(llamaindex_chunks):.0f} å­—ç¬¦")
        
    except ImportError:
        print("\nâš ï¸ LlamaIndex æœªå®‰è£…ï¼Œè·³è¿‡å¯¹æ¯”")
        llamaindex_chunks = []
    
    # æ˜¾ç¤ºå‰3å—çš„å¯¹æ¯”
    if langchain_chunks and llamaindex_chunks:
        print("\n" + "-" * 40)
        print("å‰3å—å†…å®¹å¯¹æ¯”ï¼š")
        for i in range(min(3, len(langchain_chunks), len(llamaindex_chunks))):
            print(f"\nã€ç¬¬ {i+1} å—ã€‘")
            print(f"LangChain ({len(langchain_chunks[i])} å­—ç¬¦):")
            print(f"   {langchain_chunks[i][:100]}...")
            print(f"LlamaIndex ({len(llamaindex_chunks[i])} å­—ç¬¦):")
            print(f"   {llamaindex_chunks[i][:100]}...")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--compare":
        # å¯¹æ¯”æ¨¡å¼
        compare_with_langchain()
    elif len(sys.argv) > 1 and sys.argv[1] == "--test":
        # ä»…æµ‹è¯•æŸ¥è¯¢
        test_query()
    else:
        # æ„å»ºå‘é‡æ•°æ®åº“å¹¶æµ‹è¯•
        index = build_vectorstore_llamaindex()
        test_query(index)
