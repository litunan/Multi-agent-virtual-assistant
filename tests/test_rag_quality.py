#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæ–‡æ¡£è§£æè´¨é‡æµ‹è¯•æ¨¡å—
æµ‹è¯•æŒ‡æ ‡ï¼š
1. MinerUè§£æå®Œæ•´åº¦å¯¹æ¯”
2. æ£€ç´¢ç²¾å‡†åº¦å¯¹æ¯” (LangChain vs LlamaIndex)
3. ç­”æ¡ˆè´¨é‡è¯„åˆ†
4. æ–‡æ¡£è¦†ç›–ç‡

Author: Wangwang-Agent Team
Date: 2026-01-04
"""

import os
import sys
import json
import time
import asyncio
import logging
import re
from typing import Dict, List, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, field, asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.load_key import load_key

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tests/test_results/rag_quality_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class RAGMetrics:
    """RAGè´¨é‡æŒ‡æ ‡æ±‡æ€»"""
    # æ–‡æ¡£è§£æå®Œæ•´åº¦
    total_documents: int = 0
    parsed_documents: int = 0
    parse_completeness: float = 0.0
    
    # ç»“æ„ä¿ç•™æƒ…å†µ
    tables_preserved: int = 0
    images_preserved: int = 0
    headings_preserved: int = 0
    
    # æ£€ç´¢è´¨é‡
    langchain_avg_score: float = 0.0
    llamaindex_avg_score: float = 0.0
    retrieval_speed_langchain: float = 0.0
    retrieval_speed_llamaindex: float = 0.0
    
    # ç­”æ¡ˆè´¨é‡
    keyword_hit_rate: float = 0.0
    answer_relevance_score: float = 0.0
    
    # è¯¦ç»†ç»“æœ
    test_results: List[Dict] = field(default_factory=list)


class RAGQualityTester:
    """RAGæ–‡æ¡£è§£æè´¨é‡æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.metrics = RAGMetrics()
        self.test_data_path = os.path.join(
            os.path.dirname(__file__), 'test_data'
        )
        self.results_path = os.path.join(
            os.path.dirname(__file__), 'test_results'
        )
        self.project_root = os.path.dirname(os.path.dirname(__file__))
        os.makedirs(self.results_path, exist_ok=True)
        
        self._load_test_data()

    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            with open(os.path.join(self.test_data_path, 'test_questions.json'),
                     'r', encoding='utf-8') as f:
                self.questions_data = json.load(f)
            logger.info("æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            self.questions_data = {}

    def analyze_document_parsing(self) -> Dict[str, Any]:
        """
        åˆ†æMinerUæ–‡æ¡£è§£æå®Œæ•´åº¦
        æ£€æŸ¥è§£æåçš„æ–‡æ¡£ä¿ç•™äº†å¤šå°‘ç»“æ„ä¿¡æ¯
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹åˆ†æ: MinerUæ–‡æ¡£è§£æå®Œæ•´åº¦")
        logger.info("=" * 60)
        
        rag_doc_path = os.path.join(self.project_root, 'RAG_Document')
        
        if not os.path.exists(rag_doc_path):
            logger.error(f"RAG_Documentç›®å½•ä¸å­˜åœ¨: {rag_doc_path}")
            return {'error': 'RAG_Documentç›®å½•ä¸å­˜åœ¨'}
        
        analysis_results = []
        total_tables = 0
        total_images = 0
        total_headings = 0
        total_code_blocks = 0
        total_chars = 0
        
        # éå†æ‰€æœ‰å­ç›®å½•
        for subdir in os.listdir(rag_doc_path):
            subdir_path = os.path.join(rag_doc_path, subdir)
            if not os.path.isdir(subdir_path):
                continue
            
            # æŸ¥æ‰¾full.mdæˆ–*_updated.mdæ–‡ä»¶
            md_files = [f for f in os.listdir(subdir_path) if f.endswith('.md')]
            
            for md_file in md_files:
                md_path = os.path.join(subdir_path, md_file)
                
                try:
                    with open(md_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # åˆ†ææ–‡æ¡£ç»“æ„
                    doc_analysis = self._analyze_markdown_structure(content)
                    doc_analysis['file'] = os.path.join(subdir, md_file)
                    analysis_results.append(doc_analysis)
                    
                    total_tables += doc_analysis['tables']
                    total_images += doc_analysis['images']
                    total_headings += doc_analysis['headings']
                    total_code_blocks += doc_analysis['code_blocks']
                    total_chars += doc_analysis['char_count']
                    
                    logger.info(f"  {subdir}/{md_file}:")
                    logger.info(f"    - å­—ç¬¦æ•°: {doc_analysis['char_count']}")
                    logger.info(f"    - æ ‡é¢˜æ•°: {doc_analysis['headings']}")
                    logger.info(f"    - è¡¨æ ¼æ•°: {doc_analysis['tables']}")
                    logger.info(f"    - å›¾ç‰‡æ•°: {doc_analysis['images']}")
                    
                except Exception as e:
                    logger.error(f"  è¯»å–æ–‡ä»¶å¤±è´¥ {md_path}: {e}")
        
        # æ£€æŸ¥imagesç›®å½•
        total_image_files = 0
        for subdir in os.listdir(rag_doc_path):
            images_dir = os.path.join(rag_doc_path, subdir, 'images')
            if os.path.exists(images_dir):
                image_files = [f for f in os.listdir(images_dir) 
                              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]
                total_image_files += len(image_files)
        
        # è®¡ç®—å®Œæ•´åº¦æŒ‡æ ‡
        doc_count = len(analysis_results)
        self.metrics.total_documents = doc_count
        self.metrics.parsed_documents = doc_count
        self.metrics.tables_preserved = total_tables
        self.metrics.images_preserved = total_image_files
        self.metrics.headings_preserved = total_headings
        
        # ä¼°ç®—è§£æå®Œæ•´åº¦ï¼ˆåŸºäºç»“æ„å…ƒç´ çš„ä¿ç•™æƒ…å†µï¼‰
        if doc_count > 0:
            avg_headings = total_headings / doc_count
            avg_images = total_image_files / doc_count
            # å‡è®¾ç†æƒ³çš„æ–‡æ¡£å¹³å‡åº”æœ‰5ä¸ªæ ‡é¢˜å’Œ3å¼ å›¾
            completeness = min(100, (avg_headings / 5 * 50) + (avg_images / 3 * 50))
            self.metrics.parse_completeness = completeness
        
        logger.info(f"\næ–‡æ¡£è§£æåˆ†æå®Œæˆ:")
        logger.info(f"  æ€»æ–‡æ¡£æ•°: {doc_count}")
        logger.info(f"  æ€»æ ‡é¢˜æ•°: {total_headings}")
        logger.info(f"  æ€»è¡¨æ ¼æ•°: {total_tables}")
        logger.info(f"  æ€»å›¾ç‰‡æ–‡ä»¶: {total_image_files}")
        logger.info(f"  æ€»å­—ç¬¦æ•°: {total_chars}")
        logger.info(f"  ä¼°ç®—å®Œæ•´åº¦: {self.metrics.parse_completeness:.1f}%")
        
        return {
            'test_name': 'æ–‡æ¡£è§£æå®Œæ•´åº¦åˆ†æ',
            'document_count': doc_count,
            'total_chars': total_chars,
            'total_headings': total_headings,
            'total_tables': total_tables,
            'total_images': total_image_files,
            'total_code_blocks': total_code_blocks,
            'parse_completeness': self.metrics.parse_completeness,
            'document_details': analysis_results
        }

    def _analyze_markdown_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†æMarkdownæ–‡æ¡£ç»“æ„"""
        # ç»Ÿè®¡æ ‡é¢˜
        headings = len(re.findall(r'^#{1,6}\s+', content, re.MULTILINE))
        
        # ç»Ÿè®¡è¡¨æ ¼ï¼ˆé€šè¿‡|å­—ç¬¦åˆ¤æ–­ï¼‰
        table_lines = [line for line in content.split('\n') if '|' in line and line.count('|') >= 2]
        tables = len([line for line in table_lines if '---' in line])  # è¡¨æ ¼åˆ†éš”è¡Œ
        
        # ç»Ÿè®¡å›¾ç‰‡
        images = len(re.findall(r'!\[.*?\]\(.*?\)', content))
        
        # ç»Ÿè®¡ä»£ç å—
        code_blocks = len(re.findall(r'```', content)) // 2
        
        # ç»Ÿè®¡åˆ—è¡¨é¡¹
        list_items = len(re.findall(r'^[\s]*[-*+]\s+', content, re.MULTILINE))
        list_items += len(re.findall(r'^[\s]*\d+\.\s+', content, re.MULTILINE))
        
        return {
            'char_count': len(content),
            'line_count': len(content.split('\n')),
            'headings': headings,
            'tables': tables,
            'images': images,
            'code_blocks': code_blocks,
            'list_items': list_items
        }

    def test_retrieval_comparison(self) -> Dict[str, Any]:
        """
        å¯¹æ¯”LangChainå’ŒLlamaIndexçš„æ£€ç´¢æ•ˆæœ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•: LangChain vs LlamaIndex æ£€ç´¢å¯¹æ¯”")
        logger.info("=" * 60)
        
        test_questions = self.questions_data.get('rag_test_questions', [])
        
        langchain_results = []
        llamaindex_results = []
        
        for question_data in test_questions:
            question = question_data['question']
            expected_keywords = question_data['expected_keywords']
            
            logger.info(f"\né—®é¢˜: {question[:50]}...")
            
            # æµ‹è¯•LangChainæ£€ç´¢
            try:
                lc_result = self._test_langchain_retrieval(question)
                lc_result['expected_keywords'] = expected_keywords
                lc_result['keyword_hits'] = self._count_keyword_hits(
                    lc_result.get('content', ''), expected_keywords
                )
                langchain_results.append(lc_result)
                logger.info(f"  LangChain: {lc_result['retrieval_time']:.3f}ç§’, "
                           f"å‘½ä¸­å…³é”®è¯: {lc_result['keyword_hits']}/{len(expected_keywords)}")
            except Exception as e:
                logger.error(f"  LangChainæ£€ç´¢å¤±è´¥: {e}")
                langchain_results.append({'error': str(e)})
            
            # æµ‹è¯•LlamaIndexæ£€ç´¢
            try:
                li_result = self._test_llamaindex_retrieval(question)
                li_result['expected_keywords'] = expected_keywords
                li_result['keyword_hits'] = self._count_keyword_hits(
                    li_result.get('content', ''), expected_keywords
                )
                llamaindex_results.append(li_result)
                logger.info(f"  LlamaIndex: {li_result['retrieval_time']:.3f}ç§’, "
                           f"å‘½ä¸­å…³é”®è¯: {li_result['keyword_hits']}/{len(expected_keywords)}")
            except Exception as e:
                logger.error(f"  LlamaIndexæ£€ç´¢å¤±è´¥: {e}")
                llamaindex_results.append({'error': str(e)})
        
        # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        lc_times = [r['retrieval_time'] for r in langchain_results if 'retrieval_time' in r]
        li_times = [r['retrieval_time'] for r in llamaindex_results if 'retrieval_time' in r]
        
        lc_hits = [r['keyword_hits'] for r in langchain_results if 'keyword_hits' in r]
        li_hits = [r['keyword_hits'] for r in llamaindex_results if 'keyword_hits' in r]
        
        avg_keywords = sum(len(q['expected_keywords']) for q in test_questions) / len(test_questions)
        
        self.metrics.retrieval_speed_langchain = sum(lc_times) / len(lc_times) if lc_times else 0
        self.metrics.retrieval_speed_llamaindex = sum(li_times) / len(li_times) if li_times else 0
        
        lc_hit_rate = (sum(lc_hits) / (len(lc_hits) * avg_keywords) * 100) if lc_hits else 0
        li_hit_rate = (sum(li_hits) / (len(li_hits) * avg_keywords) * 100) if li_hits else 0
        
        self.metrics.keyword_hit_rate = max(lc_hit_rate, li_hit_rate)
        
        logger.info(f"\næ£€ç´¢å¯¹æ¯”ç»“æœ:")
        logger.info(f"  LangChainå¹³å‡æ—¶é—´: {self.metrics.retrieval_speed_langchain:.3f}ç§’")
        logger.info(f"  LlamaIndexå¹³å‡æ—¶é—´: {self.metrics.retrieval_speed_llamaindex:.3f}ç§’")
        logger.info(f"  LangChainå…³é”®è¯å‘½ä¸­ç‡: {lc_hit_rate:.1f}%")
        logger.info(f"  LlamaIndexå…³é”®è¯å‘½ä¸­ç‡: {li_hit_rate:.1f}%")
        
        return {
            'test_name': 'æ£€ç´¢æ•ˆæœå¯¹æ¯”',
            'langchain_results': langchain_results,
            'llamaindex_results': llamaindex_results,
            'langchain_avg_time': self.metrics.retrieval_speed_langchain,
            'llamaindex_avg_time': self.metrics.retrieval_speed_llamaindex,
            'langchain_hit_rate': lc_hit_rate,
            'llamaindex_hit_rate': li_hit_rate
        }

    def _test_langchain_retrieval(self, query: str) -> Dict[str, Any]:
        """æµ‹è¯•LangChainæ£€ç´¢"""
        from langchain_community.vectorstores import FAISS
        from langchain_huggingface import HuggingFaceEmbeddings
        
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        vectorstore = FAISS.load_local(
            folder_path=os.path.join(self.project_root, "mcp_course_materials_db"),
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )
        
        start_time = time.time()
        docs = vectorstore.similarity_search_with_score(query, k=3)
        retrieval_time = time.time() - start_time
        
        content = "\n".join([doc.page_content for doc, score in docs])
        avg_score = sum(score for doc, score in docs) / len(docs) if docs else 0
        
        return {
            'retrieval_time': retrieval_time,
            'content': content,
            'avg_score': avg_score,
            'doc_count': len(docs)
        }

    def _test_llamaindex_retrieval(self, query: str) -> Dict[str, Any]:
        """æµ‹è¯•LlamaIndexæ£€ç´¢"""
        try:
            from llama_index.core import StorageContext, load_index_from_storage, Settings
            from llama_index.embeddings.huggingface import HuggingFaceEmbedding
            from llama_index.vector_stores.faiss import FaissVectorStore
            
            embed_model = HuggingFaceEmbedding(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                device="cpu",
                normalize=True,
            )
            Settings.embed_model = embed_model
            
            output_dir = os.path.join(self.project_root, "mcp_course_materials_db_llamaindex")
            vector_store = FaissVectorStore.from_persist_dir(output_dir)
            storage_context = StorageContext.from_defaults(
                vector_store=vector_store,
                persist_dir=output_dir,
            )
            index = load_index_from_storage(storage_context, embed_model=embed_model)
            
            retriever = index.as_retriever(similarity_top_k=3)
            
            start_time = time.time()
            nodes = retriever.retrieve(query)
            retrieval_time = time.time() - start_time
            
            content = "\n".join([node.get_content() for node in nodes])
            avg_score = sum(node.score for node in nodes if hasattr(node, 'score')) / len(nodes) if nodes else 0
            
            return {
                'retrieval_time': retrieval_time,
                'content': content,
                'avg_score': avg_score,
                'doc_count': len(nodes)
            }
        except Exception as e:
            logger.warning(f"LlamaIndexæ£€ç´¢å¤±è´¥: {e}")
            return {
                'retrieval_time': 0,
                'content': '',
                'avg_score': 0,
                'doc_count': 0,
                'error': str(e)
            }

    def _count_keyword_hits(self, content: str, keywords: List[str]) -> int:
        """ç»Ÿè®¡å…³é”®è¯å‘½ä¸­æ•°"""
        content_lower = content.lower()
        hits = sum(1 for kw in keywords if kw.lower() in content_lower)
        return hits

    async def test_answer_quality(self) -> Dict[str, Any]:
        """
        æµ‹è¯•RAGç­”æ¡ˆè´¨é‡
        ä½¿ç”¨LLMè¯„ä¼°ç­”æ¡ˆçš„ç›¸å…³æ€§å’Œå®Œæ•´æ€§
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•: RAGç­”æ¡ˆè´¨é‡è¯„ä¼°")
        logger.info("=" * 60)
        
        test_questions = self.questions_data.get('rag_test_questions', [])[:3]  # æµ‹è¯•å‰3ä¸ª
        
        quality_scores = []
        
        try:
            from enhanced_rag_agent import enhanced_rag_agent
            from langchain_core.messages import HumanMessage
            from langchain_openai import ChatOpenAI
            
            eval_model = ChatOpenAI(
                api_key=load_key("aliyun-bailian"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                model="qwen-plus",
            )
            
            for q_data in test_questions:
                question = q_data['question']
                expected_keywords = q_data['expected_keywords']
                
                logger.info(f"\nè¯„ä¼°é—®é¢˜: {question[:50]}...")
                
                try:
                    # è·å–RAGç­”æ¡ˆ
                    result = await enhanced_rag_agent.ainvoke({
                        "messages": [HumanMessage(content=question)]
                    })
                    
                    answer = ""
                    for msg in result.get('messages', []):
                        if hasattr(msg, 'content'):
                            answer = msg.content
                            break
                    
                    if not answer:
                        logger.warning("  æœªè·å–åˆ°ç­”æ¡ˆ")
                        continue
                    
                    # ä½¿ç”¨LLMè¯„ä¼°ç­”æ¡ˆè´¨é‡
                    eval_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹é—®ç­”çš„è´¨é‡ï¼Œç»™å‡º1-10çš„è¯„åˆ†ã€‚

é—®é¢˜: {question}
é¢„æœŸå…³é”®è¯: {', '.join(expected_keywords)}
ç­”æ¡ˆ: {answer[:500]}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°:
1. ç›¸å…³æ€§ - ç­”æ¡ˆæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
2. å®Œæ•´æ€§ - æ˜¯å¦åŒ…å«é¢„æœŸçš„å…³é”®ä¿¡æ¯
3. å‡†ç¡®æ€§ - ä¿¡æ¯æ˜¯å¦å‡†ç¡®

è¯·åªè¿”å›ä¸€ä¸ªæ•°å­—è¯„åˆ†(1-10)ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
                    
                    eval_response = eval_model.invoke([{"role": "user", "content": eval_prompt}])
                    
                    # æå–è¯„åˆ†
                    score_text = eval_response.content.strip()
                    score = float(re.search(r'\d+\.?\d*', score_text).group())
                    score = min(10, max(1, score))  # é™åˆ¶åœ¨1-10
                    
                    quality_scores.append(score)
                    logger.info(f"  è´¨é‡è¯„åˆ†: {score}/10")
                    
                except Exception as e:
                    logger.error(f"  è¯„ä¼°å¤±è´¥: {e}")
                
                await asyncio.sleep(0.5)
            
            avg_score = sum(quality_scores) / len(quality_scores) if quality_scores else 0
            self.metrics.answer_relevance_score = avg_score * 10  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
            
            logger.info(f"\nç­”æ¡ˆè´¨é‡è¯„ä¼°å®Œæˆ:")
            logger.info(f"  å¹³å‡è´¨é‡è¯„åˆ†: {avg_score:.1f}/10")
            logger.info(f"  ç™¾åˆ†åˆ¶è¯„åˆ†: {self.metrics.answer_relevance_score:.1f}%")
            
            return {
                'test_name': 'ç­”æ¡ˆè´¨é‡è¯„ä¼°',
                'scores': quality_scores,
                'avg_score': avg_score,
                'percentage_score': self.metrics.answer_relevance_score
            }
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            return {
                'test_name': 'ç­”æ¡ˆè´¨é‡è¯„ä¼°',
                'error': str(e)
            }

    async def run_all_tests(self) -> RAGMetrics:
        """è¿è¡Œæ‰€æœ‰RAGè´¨é‡æµ‹è¯•"""
        logger.info("\n" + "=" * 70)
        logger.info("å¼€å§‹è¿è¡Œ RAG æ–‡æ¡£è§£æè´¨é‡æµ‹è¯•")
        logger.info("=" * 70)
        
        # 1. æ–‡æ¡£è§£æå®Œæ•´åº¦åˆ†æ
        parsing_result = self.analyze_document_parsing()
        self.metrics.test_results.append(parsing_result)
        
        # 2. æ£€ç´¢æ•ˆæœå¯¹æ¯”
        retrieval_result = self.test_retrieval_comparison()
        self.metrics.test_results.append(retrieval_result)
        
        # 3. ç­”æ¡ˆè´¨é‡è¯„ä¼°
        quality_result = await self.test_answer_quality()
        self.metrics.test_results.append(quality_result)
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        # è¾“å‡ºæ±‡æ€»
        self._print_summary()
        
        return self.metrics

    def _save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        result_file = os.path.join(
            self.results_path,
            f'rag_metrics_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.metrics), f, ensure_ascii=False, indent=2)
        
        logger.info(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {result_file}")

    def _print_summary(self):
        """æ‰“å°æµ‹è¯•æ±‡æ€»"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š RAG æ–‡æ¡£è§£æè´¨é‡æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
        logger.info("=" * 70)
        
        logger.info(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ˆ ç®€å†æŒ‡æ ‡æ•°æ®                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… æ–‡æ¡£è§£æå®Œæ•´åº¦:            {self.metrics.parse_completeness:>6.1f}%                          â”‚
â”‚  âœ… å…³é”®è¯å‘½ä¸­ç‡:              {self.metrics.keyword_hit_rate:>6.1f}%                          â”‚
â”‚  âœ… ç­”æ¡ˆç›¸å…³æ€§è¯„åˆ†:            {self.metrics.answer_relevance_score:>6.1f}%                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š è§£æç»Ÿè®¡:                                                          â”‚
â”‚     - æ–‡æ¡£æ•°é‡: {self.metrics.total_documents:>3}                                               â”‚
â”‚     - ä¿ç•™æ ‡é¢˜: {self.metrics.headings_preserved:>3}                                               â”‚
â”‚     - ä¿ç•™å›¾ç‰‡: {self.metrics.images_preserved:>3}                                               â”‚
â”‚     - ä¿ç•™è¡¨æ ¼: {self.metrics.tables_preserved:>3}                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ æ£€ç´¢æ€§èƒ½:                                                          â”‚
â”‚     - LangChain: {self.metrics.retrieval_speed_langchain:.3f}ç§’                                     â”‚
â”‚     - LlamaIndex: {self.metrics.retrieval_speed_llamaindex:.3f}ç§’                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
        
        logger.info("\nğŸ“ ç®€å†æè¿°å»ºè®®:")
        logger.info(f"  - é‡‡ç”¨MinerUè¿›è¡Œé«˜ç²¾åº¦PDFè§£æï¼Œæ–‡æ¡£è§£æå®Œæ•´åº¦è¾¾åˆ° {self.metrics.parse_completeness:.0f}%")
        logger.info(f"  - åŸºäºé€’å½’åˆ‡åˆ†ç­–ç•¥ï¼Œå…³é”®ä¿¡æ¯æ£€ç´¢å‘½ä¸­ç‡ {self.metrics.keyword_hit_rate:.0f}%")
        logger.info(f"  - å¤§å¹…æå‡RAG Agentå›ç­”ç²¾å‡†åº¦ï¼Œç›¸å…³æ€§è¯„åˆ†è¾¾åˆ° {self.metrics.answer_relevance_score:.0f}%")


async def main():
    """ä¸»å‡½æ•°"""
    os.makedirs('tests/test_results', exist_ok=True)
    
    tester = RAGQualityTester()
    metrics = await tester.run_all_tests()
    
    return metrics


if __name__ == "__main__":
    asyncio.run(main())
