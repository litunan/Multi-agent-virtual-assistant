#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ™ºèƒ½ä½“è°ƒåº¦æµ‹è¯•æ¨¡å—
æµ‹è¯•æŒ‡æ ‡ï¼š
1. æ”¯æŒçš„æœ€å¤§å¯¹è¯è½®æ•°
2. è·¨Agentä»»åŠ¡æ‹†è§£æˆåŠŸç‡
3. Agentè·¯ç”±å‡†ç¡®ç‡
4. ä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›

Author: Wangwang-Agent Team
Date: 2026-01-04
"""

import os
import sys
import json
import asyncio
import time
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, field, asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.load_key import load_key

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tests/test_results/supervisor_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    success: bool
    duration: float
    details: Dict[str, Any] = field(default_factory=dict)
    error_message: str = ""


@dataclass
class SupervisorMetrics:
    """Supervisorè°ƒåº¦æŒ‡æ ‡æ±‡æ€»"""
    # å¯¹è¯è½®æ•°æµ‹è¯•
    max_conversation_rounds: int = 0
    conversation_context_retention_rate: float = 0.0
    
    # Agentè·¯ç”±æµ‹è¯•
    total_routing_tests: int = 0
    correct_routing_count: int = 0
    routing_accuracy: float = 0.0
    
    # è·¨Agentä»»åŠ¡æµ‹è¯•
    total_cross_agent_tasks: int = 0
    successful_cross_agent_tasks: int = 0
    cross_agent_success_rate: float = 0.0
    
    # æ€§èƒ½æŒ‡æ ‡
    avg_response_time: float = 0.0
    total_test_duration: float = 0.0
    
    # è¯¦ç»†ç»“æœ
    test_results: List[Dict] = field(default_factory=list)


class SupervisorMetricsTester:
    """Supervisorè°ƒåº¦æŒ‡æ ‡æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.metrics = SupervisorMetrics()
        self.test_data_path = os.path.join(
            os.path.dirname(__file__), 'test_data'
        )
        self.results_path = os.path.join(
            os.path.dirname(__file__), 'test_results'
        )
        os.makedirs(self.results_path, exist_ok=True)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        self._load_test_data()
        
    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            with open(os.path.join(self.test_data_path, 'test_questions.json'), 
                     'r', encoding='utf-8') as f:
                self.questions_data = json.load(f)
            
            with open(os.path.join(self.test_data_path, 'test_scenarios.json'), 
                     'r', encoding='utf-8') as f:
                self.scenarios_data = json.load(f)
                
            logger.info("æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            self.questions_data = {}
            self.scenarios_data = {}

    async def test_agent_routing_accuracy(self) -> TestResult:
        """
        æµ‹è¯•Agentè·¯ç”±å‡†ç¡®ç‡
        éªŒè¯Supervisorèƒ½å¦æ­£ç¡®å°†ä»»åŠ¡åˆ†é…ç»™å¯¹åº”çš„Agent
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•: Agentè·¯ç”±å‡†ç¡®ç‡")
        logger.info("=" * 60)
        
        start_time = time.time()
        correct_count = 0
        total_count = 0
        routing_details = []
        
        try:
            from enhanced_data_agent1 import professional_system_query
            
            routing_tests = self.scenarios_data.get('supervisor_routing_tests', [])
            
            for test_case in routing_tests:
                total_count += 1
                test_input = test_case['input']
                expected_agent = test_case['expected_agent']
                
                logger.info(f"\næµ‹è¯• {test_case['id']}: {test_input[:50]}...")
                
                try:
                    # æ‰§è¡ŒæŸ¥è¯¢å¹¶æ£€æµ‹å®é™…è°ƒç”¨çš„Agent
                    response = await professional_system_query(
                        test_input,
                        user_id="test_user",
                        session_id="routing_test_session"
                    )
                    
                    # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«é¢„æœŸAgentçš„ç‰¹å¾
                    # é€šè¿‡å“åº”å†…å®¹æ¨æ–­ä½¿ç”¨äº†å“ªä¸ªAgent
                    actual_agent = self._detect_agent_from_response(
                        response, test_case['category']
                    )
                    
                    is_correct = actual_agent == expected_agent
                    if is_correct:
                        correct_count += 1
                        status = "âœ… æ­£ç¡®"
                    else:
                        status = f"âŒ é”™è¯¯ (é¢„æœŸ: {expected_agent}, å®é™…: {actual_agent})"
                    
                    routing_details.append({
                        'test_id': test_case['id'],
                        'input': test_input,
                        'expected_agent': expected_agent,
                        'actual_agent': actual_agent,
                        'correct': is_correct,
                        'category': test_case['category']
                    })
                    
                    logger.info(f"  ç»“æœ: {status}")
                    
                except Exception as e:
                    logger.error(f"  æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
                    routing_details.append({
                        'test_id': test_case['id'],
                        'input': test_input,
                        'expected_agent': expected_agent,
                        'actual_agent': 'error',
                        'correct': False,
                        'error': str(e)
                    })
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.5)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
            
            self.metrics.total_routing_tests = total_count
            self.metrics.correct_routing_count = correct_count
            self.metrics.routing_accuracy = accuracy
            
            duration = time.time() - start_time
            
            logger.info(f"\nè·¯ç”±å‡†ç¡®ç‡æµ‹è¯•å®Œæˆ:")
            logger.info(f"  æ€»æµ‹è¯•æ•°: {total_count}")
            logger.info(f"  æ­£ç¡®æ•°: {correct_count}")
            logger.info(f"  å‡†ç¡®ç‡: {accuracy:.1f}%")
            
            return TestResult(
                test_name="Agentè·¯ç”±å‡†ç¡®ç‡æµ‹è¯•",
                success=accuracy >= 80,  # 80%ä»¥ä¸Šè§†ä¸ºæˆåŠŸ
                duration=duration,
                details={
                    'total_tests': total_count,
                    'correct_count': correct_count,
                    'accuracy': accuracy,
                    'routing_details': routing_details
                }
            )
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            return TestResult(
                test_name="Agentè·¯ç”±å‡†ç¡®ç‡æµ‹è¯•",
                success=False,
                duration=time.time() - start_time,
                error_message=f"å¯¼å…¥å¤±è´¥: {e}"
            )

    def _detect_agent_from_response(self, response: str, category: str) -> str:
        """æ ¹æ®å“åº”å†…å®¹æ¨æ–­ä½¿ç”¨çš„Agent"""
        response_lower = response.lower()
        
        # åœ°ç†ä½ç½®æœåŠ¡ç‰¹å¾
        if any(kw in response_lower for kw in ['å¤©æ°”', 'æ¸©åº¦', 'Â°c', 'ç»çº¬åº¦', 'åæ ‡', 'è·¯çº¿', 'å…¬é‡Œ']):
            return 'enhanced_amap_agent'
        
        # Pythonæ•°æ®åˆ†æç‰¹å¾
        if any(kw in response_lower for kw in ['import', 'print', 'plt.', 'numpy', 'pandas', 'æ‰§è¡Œç»“æœ']):
            return 'enhanced_python_agent'
        
        # RAGæ£€ç´¢ç‰¹å¾
        if any(kw in response_lower for kw in ['æŠ—ç™Œè‚½', 'è‚½', 'æœºåˆ¶', 'ç»†èƒ', 'è‚¿ç˜¤', 'ç ”ç©¶è¡¨æ˜']):
            return 'enhanced_rag_agent'
        
        # æ–‡ä»¶æ“ä½œç‰¹å¾
        if any(kw in response_lower for kw in ['æ–‡ä»¶', 'ç›®å½•', 'è¯»å–', 'åˆ›å»º', 'ä¿å­˜', 'workspace']):
            return 'safe_file_agent'
        
        # ç»ˆç«¯å‘½ä»¤ç‰¹å¾
        if any(kw in response_lower for kw in ['å‘½ä»¤', 'æ‰§è¡Œ', 'ls', 'pwd', 'è¿›ç¨‹', 'å†…å­˜']):
            return 'terminal_command_agent'
        
        # æ ¹æ®é¢„æœŸç±»åˆ«è¿”å›
        category_agent_map = {
            'åœ°ç†ä½ç½®æœåŠ¡': 'enhanced_amap_agent',
            'æ•°æ®åˆ†æ': 'enhanced_python_agent',
            'RAGæ£€ç´¢': 'enhanced_rag_agent',
            'æ–‡ä»¶æ“ä½œ': 'safe_file_agent',
            'ç»ˆç«¯å‘½ä»¤': 'terminal_command_agent'
        }
        return category_agent_map.get(category, 'unknown')

    async def test_cross_agent_task_decomposition(self) -> TestResult:
        """
        æµ‹è¯•è·¨Agentä»»åŠ¡æ‹†è§£æˆåŠŸç‡
        éªŒè¯ç³»ç»Ÿèƒ½å¦æ­£ç¡®æ‹†è§£å¹¶æ‰§è¡Œéœ€è¦å¤šä¸ªAgentåä½œçš„å¤æ‚ä»»åŠ¡
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•: è·¨Agentä»»åŠ¡æ‹†è§£")
        logger.info("=" * 60)
        
        start_time = time.time()
        successful_count = 0
        total_count = 0
        task_details = []
        
        try:
            from enhanced_data_agent1 import professional_system_query
            
            cross_agent_tests = self.questions_data.get('cross_agent_questions', [])
            
            for test_case in cross_agent_tests:
                total_count += 1
                question = test_case['question']
                expected_agents = test_case['expected_agents']
                
                logger.info(f"\næµ‹è¯• {test_case['id']}: {test_case['description']}")
                logger.info(f"  é—®é¢˜: {question[:60]}...")
                logger.info(f"  é¢„æœŸAgents: {expected_agents}")
                
                try:
                    response = await professional_system_query(
                        question,
                        user_id="test_user",
                        session_id="cross_agent_test"
                    )
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«å¤šä¸ªAgentçš„å·¥ä½œç‰¹å¾
                    detected_agents = self._detect_multiple_agents(response)
                    
                    # è®¡ç®—Agentè¦†ç›–ç‡
                    covered_agents = set(detected_agents) & set(expected_agents)
                    coverage_rate = len(covered_agents) / len(expected_agents) * 100
                    
                    is_success = coverage_rate >= 50  # è¦†ç›–50%ä»¥ä¸Šè§†ä¸ºæˆåŠŸ
                    if is_success:
                        successful_count += 1
                        status = f"âœ… æˆåŠŸ (è¦†ç›–ç‡: {coverage_rate:.0f}%)"
                    else:
                        status = f"âŒ å¤±è´¥ (è¦†ç›–ç‡: {coverage_rate:.0f}%)"
                    
                    task_details.append({
                        'test_id': test_case['id'],
                        'description': test_case['description'],
                        'expected_agents': expected_agents,
                        'detected_agents': detected_agents,
                        'coverage_rate': coverage_rate,
                        'success': is_success,
                        'response_preview': response[:200]
                    })
                    
                    logger.info(f"  æ£€æµ‹åˆ°çš„Agents: {detected_agents}")
                    logger.info(f"  ç»“æœ: {status}")
                    
                except Exception as e:
                    logger.error(f"  ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                    task_details.append({
                        'test_id': test_case['id'],
                        'description': test_case['description'],
                        'expected_agents': expected_agents,
                        'success': False,
                        'error': str(e)
                    })
                
                await asyncio.sleep(1)  # å¤æ‚ä»»åŠ¡é—´éš”é•¿ä¸€ç‚¹
            
            # è®¡ç®—æˆåŠŸç‡
            success_rate = (successful_count / total_count * 100) if total_count > 0 else 0
            
            self.metrics.total_cross_agent_tasks = total_count
            self.metrics.successful_cross_agent_tasks = successful_count
            self.metrics.cross_agent_success_rate = success_rate
            
            duration = time.time() - start_time
            
            logger.info(f"\nè·¨Agentä»»åŠ¡æµ‹è¯•å®Œæˆ:")
            logger.info(f"  æ€»ä»»åŠ¡æ•°: {total_count}")
            logger.info(f"  æˆåŠŸæ•°: {successful_count}")
            logger.info(f"  æˆåŠŸç‡: {success_rate:.1f}%")
            
            return TestResult(
                test_name="è·¨Agentä»»åŠ¡æ‹†è§£æµ‹è¯•",
                success=success_rate >= 60,
                duration=duration,
                details={
                    'total_tasks': total_count,
                    'successful_count': successful_count,
                    'success_rate': success_rate,
                    'task_details': task_details
                }
            )
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            return TestResult(
                test_name="è·¨Agentä»»åŠ¡æ‹†è§£æµ‹è¯•",
                success=False,
                duration=time.time() - start_time,
                error_message=f"å¯¼å…¥å¤±è´¥: {e}"
            )

    def _detect_multiple_agents(self, response: str) -> List[str]:
        """æ£€æµ‹å“åº”ä¸­æ¶‰åŠçš„å¤šä¸ªAgent"""
        detected = []
        response_lower = response.lower()
        
        agent_signatures = {
            'enhanced_amap_agent': ['å¤©æ°”', 'åæ ‡', 'è·¯çº¿', 'å…¬é‡Œ', 'åœ°å›¾', 'ä½ç½®'],
            'enhanced_python_agent': ['import', 'print', 'python', 'æ‰§è¡Œ', 'ä»£ç ', 'è®¡ç®—'],
            'enhanced_rag_agent': ['æŠ—ç™Œè‚½', 'è‚½', 'æœºåˆ¶', 'ç ”ç©¶', 'æ–‡çŒ®'],
            'safe_file_agent': ['æ–‡ä»¶', 'ä¿å­˜', 'åˆ›å»º', 'å†™å…¥', 'workspace'],
            'terminal_command_agent': ['å‘½ä»¤', 'ç»ˆç«¯', 'æ‰§è¡Œ', 'shell']
        }
        
        for agent, keywords in agent_signatures.items():
            if any(kw in response_lower for kw in keywords):
                detected.append(agent)
        
        return detected

    async def test_long_conversation_capability(self) -> TestResult:
        """
        æµ‹è¯•é•¿å¯¹è¯èƒ½åŠ›
        éªŒè¯ç³»ç»Ÿèƒ½å¤Ÿç»´æŒå¤šå°‘è½®æœ‰æ•ˆå¯¹è¯ï¼Œä»¥åŠä¸Šä¸‹æ–‡ä¿æŒèƒ½åŠ›
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æµ‹è¯•: é•¿å¯¹è¯èƒ½åŠ›")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        try:
            from enhanced_data_agent1 import professional_system_query
            
            conversation_context = self.questions_data.get('long_conversation_context', [])
            
            successful_rounds = 0
            context_references = 0  # ç»Ÿè®¡æˆåŠŸå¼•ç”¨ä¸Šä¸‹æ–‡çš„æ¬¡æ•°
            conversation_history = []
            
            session_id = f"long_conv_test_{int(time.time())}"
            
            for round_num, message in enumerate(conversation_context, 1):
                logger.info(f"\nç¬¬ {round_num} è½®å¯¹è¯: {message[:50]}...")
                
                try:
                    response = await professional_system_query(
                        message,
                        user_id="test_user",
                        session_id=session_id
                    )
                    
                    if response and len(response) > 20:
                        successful_rounds += 1
                        
                        # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†ä¹‹å‰çš„ä¸Šä¸‹æ–‡
                        if round_num > 1:
                            if self._check_context_reference(response, conversation_history):
                                context_references += 1
                                logger.info(f"  âœ… æˆåŠŸ (æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡å¼•ç”¨)")
                            else:
                                logger.info(f"  âœ… æˆåŠŸ")
                        else:
                            logger.info(f"  âœ… æˆåŠŸ")
                        
                        conversation_history.append({
                            'round': round_num,
                            'user': message,
                            'assistant': response[:200]
                        })
                    else:
                        logger.warning(f"  âš ï¸ å“åº”è¿‡çŸ­æˆ–ä¸ºç©º")
                        
                except Exception as e:
                    logger.error(f"  âŒ å¯¹è¯å¤±è´¥: {e}")
                    break
                
                await asyncio.sleep(0.5)
            
            # è®¡ç®—ä¸Šä¸‹æ–‡ä¿æŒç‡
            possible_references = max(successful_rounds - 1, 1)
            context_retention_rate = (context_references / possible_references * 100)
            
            self.metrics.max_conversation_rounds = successful_rounds
            self.metrics.conversation_context_retention_rate = context_retention_rate
            
            duration = time.time() - start_time
            
            logger.info(f"\né•¿å¯¹è¯æµ‹è¯•å®Œæˆ:")
            logger.info(f"  æˆåŠŸå¯¹è¯è½®æ•°: {successful_rounds}")
            logger.info(f"  ä¸Šä¸‹æ–‡ä¿æŒç‡: {context_retention_rate:.1f}%")
            
            return TestResult(
                test_name="é•¿å¯¹è¯èƒ½åŠ›æµ‹è¯•",
                success=successful_rounds >= 8,  # è‡³å°‘æ”¯æŒ8è½®
                duration=duration,
                details={
                    'successful_rounds': successful_rounds,
                    'total_rounds': len(conversation_context),
                    'context_references': context_references,
                    'context_retention_rate': context_retention_rate,
                    'conversation_history': conversation_history
                }
            )
            
        except ImportError as e:
            logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            return TestResult(
                test_name="é•¿å¯¹è¯èƒ½åŠ›æµ‹è¯•",
                success=False,
                duration=time.time() - start_time,
                error_message=f"å¯¼å…¥å¤±è´¥: {e}"
            )

    def _check_context_reference(self, response: str, history: List[Dict]) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦å¼•ç”¨äº†å†å²å¯¹è¯å†…å®¹"""
        if not history:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"ä¹‹å‰"ã€"åˆšæ‰"ã€"å‰é¢"ç­‰ä¸Šä¸‹æ–‡å¼•ç”¨è¯
        context_keywords = ['ä¹‹å‰', 'åˆšæ‰', 'å‰é¢', 'ä¸Šé¢', 'æåˆ°', 'è®¨è®º', 'è¯´è¿‡', 'åŸºäº']
        response_lower = response.lower()
        
        if any(kw in response_lower for kw in context_keywords):
            return True
        
        # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†å†å²å¯¹è¯ä¸­çš„å…³é”®è¯
        for hist in history[-3:]:  # æ£€æŸ¥æœ€è¿‘3è½®
            user_keywords = [w for w in hist['user'].split() if len(w) > 2]
            if any(kw in response for kw in user_keywords):
                return True
        
        return False

    async def run_all_tests(self) -> SupervisorMetrics:
        """è¿è¡Œæ‰€æœ‰Supervisorè°ƒåº¦æµ‹è¯•"""
        logger.info("\n" + "=" * 70)
        logger.info("å¼€å§‹è¿è¡Œ Supervisor è°ƒåº¦æŒ‡æ ‡æµ‹è¯•")
        logger.info("=" * 70)
        
        total_start = time.time()
        
        # 1. Agentè·¯ç”±å‡†ç¡®ç‡æµ‹è¯•
        routing_result = await self.test_agent_routing_accuracy()
        self.metrics.test_results.append(asdict(routing_result))
        
        # 2. è·¨Agentä»»åŠ¡æ‹†è§£æµ‹è¯•
        cross_agent_result = await self.test_cross_agent_task_decomposition()
        self.metrics.test_results.append(asdict(cross_agent_result))
        
        # 3. é•¿å¯¹è¯èƒ½åŠ›æµ‹è¯•
        long_conv_result = await self.test_long_conversation_capability()
        self.metrics.test_results.append(asdict(long_conv_result))
        
        self.metrics.total_test_duration = time.time() - total_start
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self._save_results()
        
        # è¾“å‡ºæ±‡æ€»
        self._print_summary()
        
        return self.metrics

    def _save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        result_file = os.path.join(
            self.results_path, 
            f'supervisor_metrics_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.metrics), f, ensure_ascii=False, indent=2)
        
        logger.info(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {result_file}")

    def _print_summary(self):
        """æ‰“å°æµ‹è¯•æ±‡æ€»"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š SUPERVISOR è°ƒåº¦æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
        logger.info("=" * 70)
        
        logger.info(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“ˆ ç®€å†æŒ‡æ ‡æ•°æ®                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Agentè·¯ç”±å‡†ç¡®ç‡:           {self.metrics.routing_accuracy:>6.1f}%                          â”‚
â”‚  âœ… è·¨Agentä»»åŠ¡æˆåŠŸç‡:         {self.metrics.cross_agent_success_rate:>6.1f}%                          â”‚
â”‚  âœ… æ”¯æŒå¯¹è¯è½®æ•°:              {self.metrics.max_conversation_rounds:>6}è½®                           â”‚
â”‚  âœ… ä¸Šä¸‹æ–‡ä¿æŒç‡:              {self.metrics.conversation_context_retention_rate:>6.1f}%                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â±ï¸  æ€»æµ‹è¯•æ—¶é•¿:               {self.metrics.total_test_duration:>6.1f}ç§’                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
        
        # ç”Ÿæˆç®€å†å‹å¥½çš„æè¿°
        logger.info("\nğŸ“ ç®€å†æè¿°å»ºè®®:")
        logger.info(f"  - æˆåŠŸæ”¯æŒ {self.metrics.max_conversation_rounds} è½®ä»¥ä¸Šçš„å¤æ‚é•¿å¯¹è¯")
        logger.info(f"  - å¤æ‚ä»»åŠ¡æ‹†è§£æˆåŠŸç‡è¾¾åˆ° {self.metrics.cross_agent_success_rate:.0f}%")
        logger.info(f"  - Agentæ™ºèƒ½è·¯ç”±å‡†ç¡®ç‡ {self.metrics.routing_accuracy:.0f}%")


async def main():
    """ä¸»å‡½æ•°"""
    # ç¡®ä¿æµ‹è¯•ç»“æœç›®å½•å­˜åœ¨
    os.makedirs('tests/test_results', exist_ok=True)
    
    tester = SupervisorMetricsTester()
    metrics = await tester.run_all_tests()
    
    return metrics


if __name__ == "__main__":
    asyncio.run(main())
