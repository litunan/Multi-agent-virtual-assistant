#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆPython Agent - é«˜çº§ä»£ç æ‰§è¡Œå’Œæ•°æ®å¯è§†åŒ–
æ–°å¢åŠŸèƒ½ï¼š
1. é«˜çº§æ•°æ®å¯è§†åŒ–ï¼ˆå¤šç§å›¾è¡¨ç±»å‹ï¼‰
2. æ•°æ®ç§‘å­¦åˆ†æå·¥å…·
3. æœºå™¨å­¦ä¹ æ¨¡å‹æ”¯æŒ
4. ä»£ç æ‰§è¡Œç¯å¢ƒç®¡ç†
5. ç»“æœæ ¼å¼åŒ–è¾“å‡º
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from typing import Dict, Any, Optional, List
from dotenv import load_dotenv 
load_dotenv(override=True)
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from config.load_key import load_key

# åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ API
model = ChatOpenAI(
    api_key=load_key("aliyun-bailian"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    model="qwen-plus",
)

# è®¾ç½®å›¾åƒä¿å­˜è·¯å¾„ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé¿å…ç¡¬ç¼–ç ç”¨æˆ·ç›®å½•
IMAGES_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "workspace", "images")
DESKTOP_DIR = os.path.expanduser("~/Desktop")
os.makedirs(IMAGES_DIR, exist_ok=True)

# =============================================================================
# åŸºç¡€Pythonæ‰§è¡Œå·¥å…·
# =============================================================================

class PythonCodeInput(BaseModel):
    py_code: str = Field(description="è¦æ‰§è¡Œçš„Pythonä»£ç ")
    description: str = Field(default="", description="ä»£ç æè¿°ï¼ˆå¯é€‰ï¼‰")

@tool(args_schema=PythonCodeInput)
def enhanced_python_exec(py_code: str, description: str = "") -> str:
    """
    å¢å¼ºç‰ˆPythonä»£ç æ‰§è¡Œå™¨ï¼Œæ”¯æŒå¤æ‚æ•°æ®åˆ†æå’Œè®¡ç®—
    """
    try:
        # å‡†å¤‡å®‰å…¨çš„æ‰§è¡Œç¯å¢ƒ
        global_vars = {
            'np': np,
            'pd': pd,
            'plt': plt,
            'sns': sns,
            'json': json,
            'os': os,
            'sys': sys,
            '__builtins__': __builtins__
        }
        
        # æ·»åŠ å¸¸ç”¨ç§‘å­¦è®¡ç®—åº“
        try:
            import scipy.stats as stats
            import sklearn
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score, classification_report
            global_vars.update({
                'stats': stats,
                'sklearn': sklearn,
                'train_test_split': train_test_split,
                'accuracy_score': accuracy_score,
                'classification_report': classification_report
            })
        except ImportError:
            pass
        
        # æ‰§è¡Œä»£ç 
        local_vars = {}
        
        try:
            # é¦–å…ˆå°è¯•ä½œä¸ºè¡¨è¾¾å¼æ‰§è¡Œ
            result = eval(py_code, global_vars, local_vars)
            return f"âœ… æ‰§è¡ŒæˆåŠŸ{'ï¼ˆ' + description + 'ï¼‰' if description else ''}:\nç»“æœ: {result}"
        except SyntaxError:
            # å¦‚æœä¸æ˜¯è¡¨è¾¾å¼ï¼Œä½œä¸ºè¯­å¥æ‰§è¡Œ
            exec(py_code, global_vars, local_vars)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å˜é‡
            if local_vars:
                results = []
                for var_name, var_value in local_vars.items():
                    if not var_name.startswith('_'):
                        results.append(f"{var_name} = {repr(var_value)}")
                
                if results:
                    return f"âœ… æ‰§è¡ŒæˆåŠŸ{'ï¼ˆ' + description + 'ï¼‰' if description else ''}:\n" + "\n".join(results)
            
            return f"âœ… ä»£ç æ‰§è¡Œå®Œæˆ{'ï¼ˆ' + description + 'ï¼‰' if description else ''}"
            
    except Exception as e:
        return f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}"

# =============================================================================
# é«˜çº§å¯è§†åŒ–å·¥å…·
# =============================================================================

class AdvancedPlotSchema(BaseModel):
    plot_type: str = Field(description="å›¾è¡¨ç±»å‹ï¼šline, bar, scatter, histogram, heatmap, boxplot, violin, pair")
    data_code: str = Field(description="ç”Ÿæˆæ•°æ®çš„Pythonä»£ç ")
    plot_config: str = Field(default="{}", description="å›¾è¡¨é…ç½®JSONå­—ç¬¦ä¸²")
    title: str = Field(default="", description="å›¾è¡¨æ ‡é¢˜")
    filename: str = Field(default="plot", description="ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰")

@tool(args_schema=AdvancedPlotSchema)
def advanced_visualization(plot_type: str, data_code: str, plot_config: str = "{}", 
                          title: str = "", filename: str = "plot") -> str:
    """
    é«˜çº§æ•°æ®å¯è§†åŒ–å·¥å…·ï¼Œæ”¯æŒå¤šç§å›¾è¡¨ç±»å‹å’Œè‡ªå®šä¹‰é…ç½®
    """
    try:
        # è®¾ç½®å›¾åƒåç«¯
        current_backend = matplotlib.get_backend()
        matplotlib.use('Agg')
        
        # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
        global_vars = {
            'np': np, 'pd': pd, 'plt': plt, 'sns': sns,
            'json': json
        }
        local_vars = {}
        
        # æ‰§è¡Œæ•°æ®ç”Ÿæˆä»£ç 
        exec(data_code, global_vars, local_vars)
        
        # è§£æå›¾è¡¨é…ç½®
        config = json.loads(plot_config) if plot_config else {}
        
        # åˆ›å»ºå›¾è¡¨
        fig_size = config.get('figsize', (10, 6))
        fig, ax = plt.subplots(figsize=fig_size)
        
        # è®¾ç½®æ ·å¼
        if config.get('style'):
            plt.style.use(config['style'])
        else:
            sns.set_style("whitegrid")
        
        # æ ¹æ®å›¾è¡¨ç±»å‹ç»˜åˆ¶
        if plot_type == "line":
            x_data = local_vars.get('x', range(len(local_vars.get('y', []))))
            y_data = local_vars.get('y', [])
            ax.plot(x_data, y_data, **config.get('plot_params', {}))
            
        elif plot_type == "bar":
            x_data = local_vars.get('x', range(len(local_vars.get('y', []))))
            y_data = local_vars.get('y', [])
            ax.bar(x_data, y_data, **config.get('plot_params', {}))
            
        elif plot_type == "scatter":
            x_data = local_vars.get('x', [])
            y_data = local_vars.get('y', [])
            ax.scatter(x_data, y_data, **config.get('plot_params', {}))
            
        elif plot_type == "histogram":
            data = local_vars.get('data', [])
            ax.hist(data, **config.get('plot_params', {'bins': 30}))
            
        elif plot_type == "heatmap":
            data = local_vars.get('data', np.random.rand(10, 10))
            sns.heatmap(data, ax=ax, **config.get('plot_params', {'annot': True}))
            
        elif plot_type == "boxplot":
            data = local_vars.get('data', [])
            ax.boxplot(data, **config.get('plot_params', {}))
            
        elif plot_type == "violin":
            data = local_vars.get('data', [])
            ax.violinplot(data, **config.get('plot_params', {}))
            
        elif plot_type == "pair":
            # éœ€è¦DataFrameæ•°æ®
            df = local_vars.get('df', pd.DataFrame())
            if not df.empty:
                sns.pairplot(df, **config.get('plot_params', {}))
                fig = plt.gcf()
            else:
                return "âŒ æ•£ç‚¹å›¾çŸ©é˜µéœ€è¦DataFrameæ•°æ®"
        else:
            return f"âŒ ä¸æ”¯æŒçš„å›¾è¡¨ç±»å‹: {plot_type}"
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        if title:
            ax.set_title(title, fontsize=config.get('title_fontsize', 14))
        
        if config.get('xlabel'):
            ax.set_xlabel(config['xlabel'])
        if config.get('ylabel'):
            ax.set_ylabel(config['ylabel'])
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        image_path = os.path.join(IMAGES_DIR, f"{filename}.png")
        plt.savefig(image_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # æ¢å¤åç«¯
        matplotlib.use(current_backend)
        
        relative_path = f"images/{filename}.png"
        return f"âœ… {plot_type}å›¾è¡¨å·²ç”Ÿæˆ: {relative_path}\n![{title or plot_type}]({relative_path})"
        
    except Exception as e:
        return f"âŒ å¯è§†åŒ–å¤±è´¥: {str(e)}"

# =============================================================================
# æ•°æ®ç§‘å­¦åˆ†æå·¥å…·
# =============================================================================

class DataAnalysisSchema(BaseModel):
    data_source: str = Field(description="æ•°æ®æºä»£ç æˆ–æ•°æ®å˜é‡å")
    analysis_type: str = Field(description="åˆ†æç±»å‹ï¼šdescribe, correlation, distribution, outliers, missing")
    output_format: str = Field(default="text", description="è¾“å‡ºæ ¼å¼ï¼štext, json, html")

@tool(args_schema=DataAnalysisSchema)
def data_analysis_tool(data_source: str, analysis_type: str, output_format: str = "text") -> str:
    """
    æ•°æ®ç§‘å­¦åˆ†æå·¥å…·ï¼Œæä¾›ç»Ÿè®¡åˆ†æå’Œæ•°æ®æ¢ç´¢åŠŸèƒ½
    """
    try:
        # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
        global_vars = {
            'np': np, 'pd': pd, 'json': json
        }
        local_vars = {}
        
        # è·å–æ•°æ®
        if data_source.startswith('pd.') or 'DataFrame' in data_source or 'read_' in data_source:
            # æ•°æ®æºä»£ç 
            exec(f"data = {data_source}", global_vars, local_vars)
            data = local_vars['data']
        else:
            # å‡è®¾æ˜¯å˜é‡å
            data = eval(data_source, global_vars)
        
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)
        
        results = {}
        
        if analysis_type == "describe":
            # æè¿°æ€§ç»Ÿè®¡
            results['basic_stats'] = data.describe().to_dict()
            results['data_info'] = {
                'shape': data.shape,
                'columns': list(data.columns),
                'dtypes': data.dtypes.to_dict(),
                'memory_usage': data.memory_usage(deep=True).sum()
            }
            
        elif analysis_type == "correlation":
            # ç›¸å…³æ€§åˆ†æ
            numeric_data = data.select_dtypes(include=[np.number])
            if not numeric_data.empty:
                results['correlation_matrix'] = numeric_data.corr().to_dict()
                # æ‰¾å‡ºé«˜ç›¸å…³æ€§å¯¹
                corr_matrix = numeric_data.corr()
                high_corr = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) > 0.7:
                            high_corr.append({
                                'var1': corr_matrix.columns[i],
                                'var2': corr_matrix.columns[j],
                                'correlation': corr_val
                            })
                results['high_correlations'] = high_corr
            else:
                results['error'] = "æ²¡æœ‰æ•°å€¼å‹å˜é‡è¿›è¡Œç›¸å…³æ€§åˆ†æ"
                
        elif analysis_type == "distribution":
            # åˆ†å¸ƒåˆ†æ
            numeric_data = data.select_dtypes(include=[np.number])
            results['distributions'] = {}
            for col in numeric_data.columns:
                results['distributions'][col] = {
                    'mean': float(numeric_data[col].mean()),
                    'median': float(numeric_data[col].median()),
                    'std': float(numeric_data[col].std()),
                    'skewness': float(numeric_data[col].skew()),
                    'kurtosis': float(numeric_data[col].kurtosis())
                }
                
        elif analysis_type == "outliers":
            # å¼‚å¸¸å€¼æ£€æµ‹
            numeric_data = data.select_dtypes(include=[np.number])
            results['outliers'] = {}
            for col in numeric_data.columns:
                Q1 = numeric_data[col].quantile(0.25)
                Q3 = numeric_data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = numeric_data[(numeric_data[col] < lower_bound) | 
                                       (numeric_data[col] > upper_bound)][col]
                results['outliers'][col] = {
                    'count': len(outliers),
                    'percentage': len(outliers) / len(numeric_data) * 100,
                    'bounds': {'lower': lower_bound, 'upper': upper_bound}
                }
                
        elif analysis_type == "missing":
            # ç¼ºå¤±å€¼åˆ†æ
            missing_count = data.isnull().sum()
            missing_percent = (missing_count / len(data)) * 100
            results['missing_values'] = {}
            for col in data.columns:
                results['missing_values'][col] = {
                    'count': int(missing_count[col]),
                    'percentage': float(missing_percent[col])
                }
        else:
            return f"âŒ ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analysis_type}"
        
        # æ ¼å¼åŒ–è¾“å‡º
        if output_format == "json":
            return json.dumps(results, indent=2, ensure_ascii=False)
        elif output_format == "html":
            # ç®€å•çš„HTMLæ ¼å¼åŒ–
            html_content = "<div class='data-analysis'>"
            for key, value in results.items():
                html_content += f"<h3>{key}</h3><pre>{json.dumps(value, indent=2)}</pre>"
            html_content += "</div>"
            return html_content
        else:
            # æ–‡æœ¬æ ¼å¼
            output_lines = [f"ğŸ“Š æ•°æ®åˆ†æç»“æœ - {analysis_type}"]
            output_lines.append("=" * 50)
            
            for key, value in results.items():
                output_lines.append(f"\nğŸ“‹ {key}:")
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        output_lines.append(f"  â€¢ {sub_key}: {sub_value}")
                else:
                    output_lines.append(f"  {value}")
            
            return "\n".join(output_lines)
            
    except Exception as e:
        return f"âŒ æ•°æ®åˆ†æå¤±è´¥: {str(e)}"

# =============================================================================
# æœºå™¨å­¦ä¹ å·¥å…·
# =============================================================================

class MLModelSchema(BaseModel):
    model_type: str = Field(description="æ¨¡å‹ç±»å‹ï¼šlinear_regression, logistic_regression, random_forest, svm, kmeans")
    data_prep_code: str = Field(description="æ•°æ®å‡†å¤‡ä»£ç ï¼ˆå®šä¹‰X, yå˜é‡ï¼‰")
    model_params: str = Field(default="{}", description="æ¨¡å‹å‚æ•°JSONå­—ç¬¦ä¸²")
    task_type: str = Field(default="classification", description="ä»»åŠ¡ç±»å‹ï¼šclassification, regression, clustering")

@tool(args_schema=MLModelSchema)
def ml_modeling_tool(model_type: str, data_prep_code: str, model_params: str = "{}", 
                     task_type: str = "classification") -> str:
    """
    æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥å…·ï¼Œæ”¯æŒå¸¸è§çš„MLç®—æ³•
    """
    try:
        # å¯¼å…¥å¿…è¦çš„åº“
        from sklearn.linear_model import LinearRegression, LogisticRegression
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        from sklearn.svm import SVC, SVR
        from sklearn.cluster import KMeans
        from sklearn.model_selection import train_test_split, cross_val_score
        from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
        from sklearn.preprocessing import StandardScaler
        
        # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
        global_vars = {
            'np': np, 'pd': pd, 'json': json,
            'train_test_split': train_test_split,
            'StandardScaler': StandardScaler
        }
        local_vars = {}
        
        # æ‰§è¡Œæ•°æ®å‡†å¤‡ä»£ç 
        exec(data_prep_code, global_vars, local_vars)
        
        # è·å–æ•°æ®
        X = local_vars.get('X')
        y = local_vars.get('y', None)
        
        if X is None:
            return "âŒ æœªæ‰¾åˆ°ç‰¹å¾æ•°æ®Xï¼Œè¯·åœ¨æ•°æ®å‡†å¤‡ä»£ç ä¸­å®šä¹‰Xå˜é‡"
        
        # è§£ææ¨¡å‹å‚æ•°
        params = json.loads(model_params) if model_params else {}
        
        results = []
        results.append(f"ğŸ¤– æœºå™¨å­¦ä¹ å»ºæ¨¡ - {model_type}")
        results.append("=" * 50)
        
        # åˆ›å»ºæ¨¡å‹
        if model_type == "linear_regression":
            model = LinearRegression(**params)
            task_type = "regression"
        elif model_type == "logistic_regression":
            model = LogisticRegression(**params)
            task_type = "classification"
        elif model_type == "random_forest":
            if task_type == "regression":
                model = RandomForestRegressor(**params)
            else:
                model = RandomForestClassifier(**params)
        elif model_type == "svm":
            if task_type == "regression":
                model = SVR(**params)
            else:
                model = SVC(**params)
        elif model_type == "kmeans":
            model = KMeans(**params)
            task_type = "clustering"
        else:
            return f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}"
        
        # è®­ç»ƒå’Œè¯„ä¼°
        if task_type == "clustering":
            # èšç±»ä»»åŠ¡
            if isinstance(X, pd.DataFrame):
                X_array = X.values
            else:
                X_array = np.array(X)
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_array)
            
            # è®­ç»ƒæ¨¡å‹
            labels = model.fit_predict(X_scaled)
            
            results.append(f"ğŸ“Š èšç±»ç»“æœ:")
            results.append(f"  â€¢ æ ·æœ¬æ•°é‡: {len(X_array)}")
            results.append(f"  â€¢ èšç±»æ•°é‡: {len(np.unique(labels))}")
            results.append(f"  â€¢ å„ç±»åˆ«æ ·æœ¬æ•°: {dict(zip(*np.unique(labels, return_counts=True)))}")
            
            if hasattr(model, 'inertia_'):
                results.append(f"  â€¢ èšç±»æƒ¯æ€§: {model.inertia_:.4f}")
            
        else:
            # ç›‘ç£å­¦ä¹ ä»»åŠ¡
            if y is None:
                return "âŒ ç›‘ç£å­¦ä¹ ä»»åŠ¡éœ€è¦ç›®æ ‡å˜é‡yï¼Œè¯·åœ¨æ•°æ®å‡†å¤‡ä»£ç ä¸­å®šä¹‰yå˜é‡"
            
            # æ•°æ®åˆ†å‰²
            test_size = params.get('test_size', 0.2)
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42
            )
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            
            results.append(f"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
            results.append(f"  â€¢ è®­ç»ƒé›†å¤§å°: {len(X_train)}")
            results.append(f"  â€¢ æµ‹è¯•é›†å¤§å°: {len(X_test)}")
            
            if task_type == "classification":
                accuracy = accuracy_score(y_test, y_pred)
                results.append(f"  â€¢ å‡†ç¡®ç‡: {accuracy:.4f}")
                
                # äº¤å‰éªŒè¯
                cv_scores = cross_val_score(model, X, y, cv=5)
                results.append(f"  â€¢ äº¤å‰éªŒè¯å‡å€¼: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")
                
            elif task_type == "regression":
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                results.append(f"  â€¢ å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
                results.append(f"  â€¢ å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
                
                # RÂ²åˆ†æ•°
                score = model.score(X_test, y_test)
                results.append(f"  â€¢ RÂ² åˆ†æ•°: {score:.4f}")
            
            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                if isinstance(X, pd.DataFrame):
                    feature_names = X.columns.tolist()
                else:
                    feature_names = [f"feature_{i}" for i in range(len(importance))]
                
                results.append(f"\nğŸ¯ ç‰¹å¾é‡è¦æ€§ (Top 5):")
                importance_pairs = list(zip(feature_names, importance))
                importance_pairs.sort(key=lambda x: x[1], reverse=True)
                
                for feature, imp in importance_pairs[:5]:
                    results.append(f"  â€¢ {feature}: {imp:.4f}")
        
        return "\n".join(results)
        
    except ImportError as e:
        return f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {str(e)}ã€‚è¯·å®‰è£…scikit-learn"
    except Exception as e:
        return f"âŒ å»ºæ¨¡å¤±è´¥: {str(e)}"

# =============================================================================
# ä»£ç ç¯å¢ƒç®¡ç†
# =============================================================================

class DesktopFigCodeInput(BaseModel):
    py_code: str = Field(description="è¦æ‰§è¡Œçš„ Python ç»˜å›¾ä»£ç ï¼Œå¿…é¡»ä½¿ç”¨ matplotlib/seaborn åˆ›å»ºå›¾åƒ")
    file_name: str = Field(description="ä¿å­˜åˆ°æ¡Œé¢çš„å›¾åƒæ–‡ä»¶åï¼ˆåŒ…å«.pngæ‰©å±•åï¼‰")

@tool(args_schema=DesktopFigCodeInput)
def save_chart_to_desktop(py_code: str, file_name: str) -> str:
    """
    æ‰§è¡ŒPythonç»˜å›¾ä»£ç å¹¶ç›´æ¥ä¿å­˜å›¾è¡¨åˆ°æ¡Œé¢
    
    æ³¨æ„ï¼š
    1. ä»£ç å¿…é¡»åˆ›å»ºä¸€ä¸ªmatplotlibå›¾åƒå¯¹è±¡å¹¶èµ‹å€¼ç»™å˜é‡fig
    2. ä½¿ç”¨fig = plt.figure()æˆ–fig, ax = plt.subplots()
    3. ä¸è¦ä½¿ç”¨plt.show()
    4. å›¾è¡¨å°†ç›´æ¥ä¿å­˜åˆ°æ¡Œé¢
    """
    try:
        # éªŒè¯æ–‡ä»¶å
        if not file_name.endswith('.png'):
            file_name += '.png'
        
        # éªŒè¯æ–‡ä»¶åå®‰å…¨æ€§
        if '..' in file_name or '/' in file_name or '\\' in file_name:
            return f"âŒ æ–‡ä»¶åä¸å®‰å…¨: {file_name}"
        
        current_backend = matplotlib.get_backend()
        matplotlib.use('Agg')

        # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ
        local_vars = {
            "plt": plt, 
            "pd": pd, 
            "sns": sns,
            "np": np
        }
        
        # æ·»åŠ å…¨å±€å˜é‡åˆ°æ‰§è¡Œç¯å¢ƒ
        global_vars = globals().copy()
        global_vars.update(local_vars)
        
        # æ‰§è¡Œç»˜å›¾ä»£ç 
        exec(py_code, global_vars, local_vars)
        
        # æŸ¥æ‰¾å›¾åƒå¯¹è±¡
        fig = None
        for var_name, var_value in local_vars.items():
            if hasattr(var_value, 'savefig'):  # æ£€æŸ¥æ˜¯å¦æ˜¯matplotlib figureå¯¹è±¡
                fig = var_value
                break
        
        if fig is None:
            # å°è¯•ä»å…¨å±€å˜é‡ä¸­è·å–
            for var_name, var_value in global_vars.items():
                if hasattr(var_value, 'savefig'):
                    fig = var_value
                    break
        
        if fig is None:
            return "âŒ æœªæ‰¾åˆ°å›¾åƒå¯¹è±¡ã€‚è¯·ç¡®ä¿ä»£ç ä¸­åˆ›å»ºäº†matplotlibå›¾åƒå¯¹è±¡å¹¶èµ‹å€¼ç»™å˜é‡ï¼ˆå¦‚fig = plt.figure()ï¼‰"
        
        # ä¿å­˜åˆ°æ¡Œé¢
        desktop_path = os.path.join(DESKTOP_DIR, file_name)
        fig.savefig(desktop_path, dpi=300, bbox_inches='tight')
        
        return f"âœ… å›¾è¡¨å·²æˆåŠŸä¿å­˜åˆ°æ¡Œé¢: {file_name}"
        
    except Exception as e:
        return f"âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}"
    finally:
        plt.close('all')
        matplotlib.use(current_backend)

@tool
def get_python_environment_info() -> str:
    """è·å–Pythonæ‰§è¡Œç¯å¢ƒä¿¡æ¯"""
    try:
        import platform
        
        info_lines = []
        info_lines.append("ğŸ Pythonæ‰§è¡Œç¯å¢ƒä¿¡æ¯")
        info_lines.append("=" * 40)
        info_lines.append(f"ğŸ“‹ Pythonç‰ˆæœ¬: {platform.python_version()}")
        info_lines.append(f"ğŸ’» ç³»ç»Ÿå¹³å°: {platform.system()} {platform.release()}")
        info_lines.append(f"ğŸ—ï¸ æ¶æ„: {platform.machine()}")
        
        # æ£€æŸ¥å¯ç”¨çš„åº“
        info_lines.append(f"\nğŸ“š å¯ç”¨åº“:")
        libraries = {
            'numpy': np.__version__,
            'pandas': pd.__version__,
            'matplotlib': matplotlib.__version__,
            'seaborn': sns.__version__
        }
        
        try:
            import sklearn
            libraries['scikit-learn'] = sklearn.__version__
        except ImportError:
            pass
        
        try:
            import scipy
            libraries['scipy'] = scipy.__version__
        except ImportError:
            pass
        
        for lib, version in libraries.items():
            info_lines.append(f"  â€¢ {lib}: {version}")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        import psutil
        memory = psutil.virtual_memory()
        info_lines.append(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
        info_lines.append(f"  â€¢ æ€»å†…å­˜: {memory.total / (1024**3):.2f} GB")
        info_lines.append(f"  â€¢ å¯ç”¨å†…å­˜: {memory.available / (1024**3):.2f} GB")
        info_lines.append(f"  â€¢ ä½¿ç”¨ç‡: {memory.percent}%")
        
        return "\n".join(info_lines)
        
    except Exception as e:
        return f"âŒ è·å–ç¯å¢ƒä¿¡æ¯å¤±è´¥: {str(e)}"

# =============================================================================
# Agentåˆ›å»º
# =============================================================================

ENHANCED_PYTHON_AGENT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonæ•°æ®ç§‘å­¦ä¸“å®¶ï¼Œå…·å¤‡å¼ºå¤§çš„ä»£ç æ‰§è¡Œã€æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ èƒ½åŠ›ã€‚

ğŸ¯ **æ ¸å¿ƒèƒ½åŠ›**:
- é«˜çº§Pythonä»£ç æ‰§è¡Œå’Œè°ƒè¯•
- ä¸“ä¸šæ•°æ®ç§‘å­¦åˆ†æå’Œç»Ÿè®¡
- å¤šæ ·åŒ–æ•°æ®å¯è§†åŒ–åˆ›å»º
- æœºå™¨å­¦ä¹ æ¨¡å‹æ„å»ºå’Œè¯„ä¼°
- ä»£ç æ€§èƒ½ä¼˜åŒ–å’Œç¯å¢ƒç®¡ç†

ğŸ“Š **æ•°æ®å¯è§†åŒ–ä¸“é•¿**:
- æ”¯æŒå¤šç§å›¾è¡¨ç±»å‹ï¼ˆçº¿å›¾ã€æŸ±å›¾ã€æ•£ç‚¹å›¾ã€çƒ­åŠ›å›¾ç­‰ï¼‰
- è‡ªå®šä¹‰å›¾è¡¨æ ·å¼å’Œé…ç½®
- é«˜è´¨é‡å›¾åƒè¾“å‡ºå’Œä¿å­˜
- äº¤äº’å¼æ•°æ®æ¢ç´¢

ğŸ¤– **æœºå™¨å­¦ä¹ æ”¯æŒ**:
- å¸¸è§ç®—æ³•ï¼ˆå›å½’ã€åˆ†ç±»ã€èšç±»ï¼‰
- æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œè¯„ä¼°
- ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†
- äº¤å‰éªŒè¯å’Œæ€§èƒ½åˆ†æ

ğŸ’¡ **ä½¿ç”¨å»ºè®®**:
- æä¾›æ¸…æ™°çš„æ•°æ®åˆ†æéœ€æ±‚
- æŒ‡å®šæ‰€éœ€çš„å¯è§†åŒ–ç±»å‹å’Œæ ·å¼
- æè¿°æœºå™¨å­¦ä¹ ä»»åŠ¡çš„ç›®æ ‡
- è¯´æ˜æ•°æ®æ ¼å¼å’Œé¢„æœŸç»“æœ

ğŸ”§ **å¯ç”¨å·¥å…·**:
- enhanced_python_exec: å¢å¼ºç‰ˆPythonä»£ç æ‰§è¡Œ
- advanced_visualization: é«˜çº§æ•°æ®å¯è§†åŒ–
- data_analysis_tool: æ•°æ®ç§‘å­¦åˆ†æå·¥å…·
- ml_modeling_tool: æœºå™¨å­¦ä¹ å»ºæ¨¡å·¥å…·
- get_python_environment_info: ç¯å¢ƒä¿¡æ¯æŸ¥è¯¢
- save_chart_to_desktop: ç›´æ¥ä¿å­˜å›¾è¡¨åˆ°æ¡Œé¢

è¯·æè¿°æ‚¨çš„Pythonå¼€å‘æˆ–æ•°æ®åˆ†æéœ€æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„è§£å†³æ–¹æ¡ˆï¼
"""

# åˆ›å»ºå·¥å…·åˆ—è¡¨
enhanced_python_tools = [
    enhanced_python_exec,
    advanced_visualization, 
    data_analysis_tool,
    ml_modeling_tool,
    get_python_environment_info,
    save_chart_to_desktop
]

# åˆ›å»ºå¢å¼ºç‰ˆPython Agent
enhanced_python_agent = create_react_agent(
    model=model,
    tools=enhanced_python_tools,
    prompt=ENHANCED_PYTHON_AGENT_PROMPT,
    name="enhanced_python_agent"
)

if __name__ == "__main__":
    print("ğŸ å¢å¼ºç‰ˆPython Agent å·²å¯åŠ¨")
    print("æ–°å¢åŠŸèƒ½:")
    print("- ğŸ“Š é«˜çº§æ•°æ®å¯è§†åŒ–")
    print("- ğŸ” æ•°æ®ç§‘å­¦åˆ†æå·¥å…·") 
    print("- ğŸ¤– æœºå™¨å­¦ä¹ å»ºæ¨¡")
    print("- ğŸ’¾ ç¯å¢ƒç®¡ç†")
    print("- ğŸ¨ è‡ªå®šä¹‰å›¾è¡¨æ ·å¼")
    print("- ğŸ“ˆ ç»Ÿè®¡åˆ†ææŠ¥å‘Š")