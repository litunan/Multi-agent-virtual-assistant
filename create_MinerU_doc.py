#!/usr/bin/env python3
"""
MinerUæ–‡æ¡£æ‰¹é‡å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. æ‰¹é‡å¤„ç†æ€»ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
2. å°†æ¯ä¸ªå­ç›®å½•çš„å›¾ç‰‡å¤åˆ¶åˆ°å‰ç«¯publicç›®å½•çš„ç›¸åº”å­ç›®å½•
3. ä¿®æ”¹MDæ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„å¼•ç”¨
4. å°†å¤„ç†åçš„MDå†…å®¹æ·»åŠ åˆ°å‘é‡åº“

Author: AI Assistant
Date: 2025-01-31
"""

import os
import shutil
from pathlib import Path
import re
from typing import List
from dotenv import load_dotenv
from langchain_community.embeddings import DashScopeEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥LangChainç›¸å…³åŒ…
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# é…ç½®å‚æ•°
MINERU_DIR = "RAG_Document"
FRONTEND_PUBLIC_DIR = "agent-chat-ui-main/public"
VECTORSTORE_PATH = "mcp_course_materials_db"
MD_FILE = "full.md"
IMAGES_DIR = "images"

# æ–°çš„å›¾ç‰‡è·¯å¾„å‰ç¼€ï¼ˆå‰ç«¯è®¿é—®è·¯å¾„ï¼‰
IMAGE_PREFIX = "/langchain-course-images"


def setup_directories_for_subdir(subdir_name: str) -> Path:
    """ä¸ºå­ç›®å½•åˆ›å»ºå‰ç«¯å›¾ç‰‡ç›®å½•"""
    frontend_images_dir = Path(FRONTEND_PUBLIC_DIR) / "langchain-course-images" / subdir_name
    frontend_images_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… åˆ›å»ºå‰ç«¯å›¾ç‰‡å­ç›®å½•: {frontend_images_dir}")
    return frontend_images_dir


def copy_images_to_frontend(source_dir: Path, target_dir: Path) -> List[str]:
    """å¤åˆ¶å›¾ç‰‡åˆ°å‰ç«¯ç›®å½•"""
    copied_images = []
    source_images_dir = source_dir / IMAGES_DIR

    if not source_images_dir.exists():
        print(f"âš ï¸  å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {source_images_dir}")
        return copied_images

    # æ”¯æŒå¤šç§å›¾ç‰‡æ ¼å¼
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp', '*.webp']

    for extension in image_extensions:
        for image_file in source_images_dir.glob(extension):
            target_file = target_dir / image_file.name
            shutil.copy2(image_file, target_file)
            copied_images.append(image_file.name)
            print(f"ğŸ“· å¤åˆ¶å›¾ç‰‡: {image_file.name}")

    print(f"âœ… æ€»å…±å¤åˆ¶äº† {len(copied_images)} å¼ å›¾ç‰‡")
    return copied_images


def update_image_paths_in_md(md_content: str, subdir_name: str) -> str:
    """æ›´æ–°MDæ–‡ä»¶ä¸­çš„å›¾ç‰‡è·¯å¾„ï¼ŒåŒ…å«å­ç›®å½•ä¿¡æ¯"""
    # åŒ¹é…å›¾ç‰‡å¼•ç”¨æ ¼å¼: ![](images/filename.jpg)
    pattern = r'!\[\]\(images/([^)]+)\)'

    def replace_image_path(match):
        filename = match.group(1)
        new_path = f"{IMAGE_PREFIX}/{subdir_name}/{filename}"
        return f"![]({new_path})"

    updated_content = re.sub(pattern, replace_image_path, md_content)

    # ç»Ÿè®¡æ›¿æ¢çš„å›¾ç‰‡æ•°é‡
    original_count = len(re.findall(pattern, md_content))
    print(f"ğŸ”„ æ›´æ–°äº† {original_count} ä¸ªå›¾ç‰‡è·¯å¾„å¼•ç”¨ï¼Œä½¿ç”¨å­ç›®å½•: {subdir_name}")

    return updated_content


def split_markdown_content(content: str, subdir_name: str) -> List[Document]:
    """åˆ†å‰²markdownå†…å®¹ä¸ºæ–‡æ¡£å—ï¼Œå¹¶æ·»åŠ å­ç›®å½•ä¿¡æ¯"""
    # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=[
            "\n# ",  # ä¸€çº§æ ‡é¢˜
            "\n## ",  # äºŒçº§æ ‡é¢˜
            "\n### ",  # ä¸‰çº§æ ‡é¢˜
            "\n\n",  # æ®µè½
            "\n",  # è¡Œ
            " ",  # è¯
            ""  # å­—ç¬¦
        ]
    )

    # åˆ†å‰²æ–‡æœ¬
    texts = text_splitter.split_text(content)

    # åˆ›å»ºDocumentå¯¹è±¡
    documents = []
    for i, text in enumerate(texts):
        doc = Document(
            page_content=text,
            metadata={
                "source": "ACPè®ºæ–‡",
                "chunk_id": i,
                "document_type": "course_material",
                "processed_by": "MinerU",
                "subdirectory": subdir_name  # æ·»åŠ å­ç›®å½•ä¿¡æ¯
            }
        )
        documents.append(doc)

    print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²ä¸º {len(documents)} ä¸ªå—")
    return documents


def add_to_vectorstore(documents: List[Document]):
    """å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“"""
    try:
        # åˆå§‹åŒ–embeddings
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v4",
            dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")
        )

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰å‘é‡åº“
        vectorstore_path = Path(VECTORSTORE_PATH)

        if vectorstore_path.exists():
            print("ğŸ“š åŠ è½½ç°æœ‰å‘é‡åº“...")
            # åŠ è½½ç°æœ‰å‘é‡åº“
            vectorstore = FAISS.load_local(
                folder_path=VECTORSTORE_PATH,
                embeddings=embeddings,
                allow_dangerous_deserialization=True,
            )

            # æ·»åŠ æ–°æ–‡æ¡£
            vectorstore.add_documents(documents)
            print("âœ… æ–°æ–‡æ¡£å·²æ·»åŠ åˆ°ç°æœ‰å‘é‡åº“")

        else:
            print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡åº“...")
            # åˆ›å»ºæ–°çš„å‘é‡åº“
            vectorstore = FAISS.from_documents(documents, embeddings)
            print("âœ… æ–°å‘é‡åº“åˆ›å»ºæˆåŠŸ")

        # ä¿å­˜å‘é‡åº“
        vectorstore.save_local(VECTORSTORE_PATH)
        print(f"ğŸ’¾ å‘é‡åº“å·²ä¿å­˜åˆ°: {VECTORSTORE_PATH}")

        # æ˜¾ç¤ºå‘é‡åº“ä¿¡æ¯
        print(f"ğŸ“Š å‘é‡åº“æ€»æ–‡æ¡£æ•°: {vectorstore.index.ntotal}")

    except Exception as e:
        print(f"âŒ å‘é‡åº“æ“ä½œå¤±è´¥: {str(e)}")
        raise


def validate_environment():
    """éªŒè¯ç¯å¢ƒé…ç½®"""
    required_keys = ["DASHSCOPE_API_KEY"]
    missing_keys = []

    for key in required_keys:
        if not os.getenv(key):
            missing_keys.append(key)

    if missing_keys:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_keys)}")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡")
        return False

    print("âœ… ç¯å¢ƒå˜é‡éªŒè¯é€šè¿‡")
    return True


def process_single_subdirectory(subdir_path: Path, subdir_name: str):
    """å¤„ç†å•ä¸ªå­ç›®å½•"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ”„ å¤„ç†å­ç›®å½•: {subdir_name}")
    print(f"{'=' * 50}")

    md_file_path = subdir_path / MD_FILE

    if not md_file_path.exists():
        print(f"â­ï¸  å­ç›®å½• {subdir_name} ä¸­æ‰¾ä¸åˆ° {MD_FILE}ï¼Œè·³è¿‡")
        return 0, 0

    try:
        # 1. è®¾ç½®ç›®å½•ï¼ˆä¸ºå­ç›®å½•åˆ›å»ºå¯¹åº”çš„å‰ç«¯å›¾ç‰‡ç›®å½•ï¼‰
        frontend_images_dir = setup_directories_for_subdir(subdir_name)

        # 2. å¤åˆ¶å›¾ç‰‡åˆ°å‰ç«¯
        copied_images = copy_images_to_frontend(subdir_path, frontend_images_dir)

        # 3. è¯»å–å’Œå¤„ç†MDæ–‡ä»¶
        print(f"ğŸ“– è¯»å–MDæ–‡ä»¶: {md_file_path}")
        with open(md_file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()

        # 4. æ›´æ–°å›¾ç‰‡è·¯å¾„ï¼ˆåŒ…å«å­ç›®å½•ä¿¡æ¯ï¼‰
        updated_content = update_image_paths_in_md(md_content, subdir_name)

        # 5. ä¿å­˜æ›´æ–°åçš„MDæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        updated_md_path = subdir_path / f"{subdir_name}_updated.md"
        with open(updated_md_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        print(f"ğŸ’¾ ä¿å­˜æ›´æ–°åçš„MDæ–‡ä»¶: {updated_md_path}")

        # 6. åˆ†å‰²å†…å®¹
        documents = split_markdown_content(updated_content, subdir_name)

        # 7. æ·»åŠ åˆ°å‘é‡åº“
        add_to_vectorstore(documents)

        print(f"âœ… å­ç›®å½• {subdir_name} å¤„ç†å®Œæˆ")
        return len(copied_images), len(documents)

    except Exception as e:
        print(f"âŒ å¤„ç†å­ç›®å½• {subdir_name} å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 0, 0


def main():
    """ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ‰€æœ‰å­ç›®å½•"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†MinerUæ–‡æ¡£...")

    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        return

    # æ£€æŸ¥æ€»ç›®å½•
    base_dir = Path(MINERU_DIR)
    if not base_dir.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ€»ç›®å½•: {base_dir}")
        return

    # è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [d for d in base_dir.iterdir() if d.is_dir()]
    if not subdirs:
        print(f"âŒ åœ¨æ€»ç›®å½•ä¸‹æ‰¾ä¸åˆ°å­ç›®å½•: {base_dir}")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(subdirs)} ä¸ªå­ç›®å½•")

    total_copied_images = 0
    total_documents = 0
    processed_dirs = []

    for subdir in subdirs:
        subdir_name = subdir.name
        copied_images, documents_count = process_single_subdirectory(subdir, subdir_name)

        if documents_count > 0:  # åªæœ‰æˆåŠŸå¤„ç†çš„ç›®å½•æ‰è®¡å…¥ç»Ÿè®¡
            total_copied_images += copied_images
            total_documents += documents_count
            processed_dirs.append(subdir_name)

    print(f"\n{'ğŸ‰' * 3} æ‰¹é‡å¤„ç†å®Œæˆï¼ {'ğŸ‰' * 3}")
    print(f"ğŸ“ æ€»ç»“:")
    print(f"   - æˆåŠŸå¤„ç†å­ç›®å½•: {len(processed_dirs)} ä¸ª")
    print(f"   - æ€»å¤åˆ¶å›¾ç‰‡: {total_copied_images} å¼ ")
    print(f"   - æ€»æ–‡æ¡£åˆ†å—: {total_documents} ä¸ª")
    print(f"   - å‘é‡åº“è·¯å¾„: {VECTORSTORE_PATH}")
    if processed_dirs:
        print(f"   - å·²å¤„ç†çš„å­ç›®å½•: {', '.join(processed_dirs)}")


if __name__ == "__main__":
    main()