#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯” LangChain å’Œ LlamaIndex ä¸¤ç§åˆ†å—æ–¹å¼çš„æ£€ç´¢æ•ˆæœ

æµ‹è¯•ç»´åº¦ï¼š
1. æ£€ç´¢é€Ÿåº¦
2. æ£€ç´¢ç›¸å…³æ€§
3. ä¸Šä¸‹æ–‡å®Œæ•´æ€§
4. æœ€ç»ˆç­”æ¡ˆè´¨é‡
"""

import sys
import os
import time
from typing import List, Dict, Any

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from config.load_key import load_key


def test_langchain_retrieval(query: str, top_k: int = 3) -> Dict[str, Any]:
    """æµ‹è¯• LangChain ç‰ˆæœ¬çš„æ£€ç´¢"""
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    
    # åŠ è½½ embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    vectorstore = FAISS.load_local(
        folder_path="mcp_course_materials_db",
        embeddings=embeddings,
        allow_dangerous_deserialization=True,
    )
    
    # æ‰§è¡Œæ£€ç´¢
    start_time = time.time()
    docs = vectorstore.similarity_search_with_score(query, k=top_k)
    retrieval_time = time.time() - start_time
    
    # æ•´ç†ç»“æœ
    results = []
    for doc, score in docs:
        results.append({
            'content': doc.page_content,
            'score': float(score),
            'length': len(doc.page_content),
            'metadata': doc.metadata
        })
    
    return {
        'method': 'LangChain',
        'retrieval_time': retrieval_time,
        'results': results,
        'total_length': sum(r['length'] for r in results),
        'avg_chunk_size': sum(r['length'] for r in results) / len(results) if results else 0
    }


def test_llamaindex_retrieval(query: str, top_k: int = 3) -> Dict[str, Any]:
    """æµ‹è¯• LlamaIndex ç‰ˆæœ¬çš„æ£€ç´¢"""
    from llama_index.core import StorageContext, load_index_from_storage, Settings
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.vector_stores.faiss import FaissVectorStore
    
    # åŠ è½½ embeddings
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        device="cpu",
        normalize=True,
    )
    Settings.embed_model = embed_model
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    output_dir = "mcp_course_materials_db_llamaindex"
    vector_store = FaissVectorStore.from_persist_dir(output_dir)
    storage_context = StorageContext.from_defaults(
        vector_store=vector_store,
        persist_dir=output_dir,
    )
    index = load_index_from_storage(storage_context, embed_model=embed_model)
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = index.as_retriever(similarity_top_k=top_k)
    
    # æ‰§è¡Œæ£€ç´¢
    start_time = time.time()
    nodes = retriever.retrieve(query)
    retrieval_time = time.time() - start_time
    
    # æ•´ç†ç»“æœ
    results = []
    for node in nodes:
        results.append({
            'content': node.get_content(),
            'score': float(node.score) if hasattr(node, 'score') else 0.0,
            'length': len(node.get_content()),
            'metadata': node.metadata
        })
    
    return {
        'method': 'LlamaIndex',
        'retrieval_time': retrieval_time,
        'results': results,
        'total_length': sum(r['length'] for r in results),
        'avg_chunk_size': sum(r['length'] for r in results) / len(results) if results else 0
    }


def generate_answer_with_context(query: str, context: str, method: str) -> Dict[str, Any]:
    """ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
    from langchain_openai import ChatOpenAI
    
    model = ChatOpenAI(
        api_key=load_key("aliyun-bailian"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model="qwen-plus",
    )
    
    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ—ç™Œè‚½ç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„è§£ç­”ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{query}

æ£€ç´¢ä¸Šä¸‹æ–‡ï¼š
{context}

è¯·æä¾›ä¸“ä¸šçš„ç§‘å­¦è§£ç­”ï¼š"""
    
    start_time = time.time()
    response = model.invoke([{"role": "user", "content": prompt}])
    generation_time = time.time() - start_time
    
    return {
        'method': method,
        'answer': response.content,
        'generation_time': generation_time,
        'answer_length': len(response.content)
    }


def compare_retrieval_quality(query: str):
    """å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„æ£€ç´¢è´¨é‡"""
    
    print("=" * 80)
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print("=" * 80)
    
    # æµ‹è¯• LangChain
    print("\nğŸ“¦ æµ‹è¯• LangChain æ£€ç´¢...")
    langchain_result = test_langchain_retrieval(query, top_k=3)
    
    # æµ‹è¯• LlamaIndex
    print("ğŸ“¦ æµ‹è¯• LlamaIndex æ£€ç´¢...")
    llamaindex_result = test_llamaindex_retrieval(query, top_k=3)
    
    # å¯¹æ¯”æ£€ç´¢é€Ÿåº¦
    print("\n" + "=" * 80)
    print("âš¡ æ£€ç´¢é€Ÿåº¦å¯¹æ¯”")
    print("=" * 80)
    print(f"LangChain:  {langchain_result['retrieval_time']:.4f} ç§’")
    print(f"LlamaIndex: {llamaindex_result['retrieval_time']:.4f} ç§’")
    speed_winner = "LangChain" if langchain_result['retrieval_time'] < llamaindex_result['retrieval_time'] else "LlamaIndex"
    print(f"âœ… é€Ÿåº¦ä¼˜èƒœ: {speed_winner}")
    
    # å¯¹æ¯”æ£€ç´¢ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ£€ç´¢ç»“æœç»Ÿè®¡")
    print("=" * 80)
    
    print(f"\nã€LangChainã€‘")
    print(f"  - æ£€ç´¢å—æ•°: {len(langchain_result['results'])}")
    print(f"  - å¹³å‡å—å¤§å°: {langchain_result['avg_chunk_size']:.0f} å­—ç¬¦")
    print(f"  - æ€»ä¸Šä¸‹æ–‡é•¿åº¦: {langchain_result['total_length']} å­—ç¬¦")
    print(f"  - å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {sum(r['score'] for r in langchain_result['results']) / len(langchain_result['results']):.4f}")
    
    print(f"\nã€LlamaIndexã€‘")
    print(f"  - æ£€ç´¢å—æ•°: {len(llamaindex_result['results'])}")
    print(f"  - å¹³å‡å—å¤§å°: {llamaindex_result['avg_chunk_size']:.0f} å­—ç¬¦")
    print(f"  - æ€»ä¸Šä¸‹æ–‡é•¿åº¦: {llamaindex_result['total_length']} å­—ç¬¦")
    print(f"  - å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {sum(r['score'] for r in llamaindex_result['results']) / len(llamaindex_result['results']):.4f}")
    
    # æ˜¾ç¤ºæ£€ç´¢å†…å®¹é¢„è§ˆ
    print("\n" + "=" * 80)
    print("ğŸ“„ æ£€ç´¢å†…å®¹é¢„è§ˆï¼ˆå‰3å—ï¼‰")
    print("=" * 80)
    
    for i in range(min(3, len(langchain_result['results']))):
        print(f"\nã€ç¬¬ {i+1} å—å¯¹æ¯”ã€‘")
        print(f"\nLangChain ({langchain_result['results'][i]['length']} å­—ç¬¦, åˆ†æ•°: {langchain_result['results'][i]['score']:.4f}):")
        print(f"  {langchain_result['results'][i]['content'][:200]}...")
        
        print(f"\nLlamaIndex ({llamaindex_result['results'][i]['length']} å­—ç¬¦, åˆ†æ•°: {llamaindex_result['results'][i]['score']:.4f}):")
        print(f"  {llamaindex_result['results'][i]['content'][:200]}...")
    
    # ç”Ÿæˆç­”æ¡ˆå¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ’¬ ç”Ÿæˆç­”æ¡ˆå¯¹æ¯”")
    print("=" * 80)
    
    print("\nğŸ¤– ä½¿ç”¨ LangChain ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ...")
    langchain_context = "\n\n".join([r['content'] for r in langchain_result['results']])
    langchain_answer = generate_answer_with_context(query, langchain_context, "LangChain")
    
    print("ğŸ¤– ä½¿ç”¨ LlamaIndex ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ...")
    llamaindex_context = "\n\n".join([r['content'] for r in llamaindex_result['results']])
    llamaindex_answer = generate_answer_with_context(query, llamaindex_context, "LlamaIndex")
    
    print(f"\nã€LangChain ç­”æ¡ˆã€‘({langchain_answer['answer_length']} å­—ç¬¦, è€—æ—¶: {langchain_answer['generation_time']:.2f}s)")
    print("-" * 80)
    print(langchain_answer['answer'])
    
    print(f"\nã€LlamaIndex ç­”æ¡ˆã€‘({llamaindex_answer['answer_length']} å­—ç¬¦, è€—æ—¶: {llamaindex_answer['generation_time']:.2f}s)")
    print("-" * 80)
    print(llamaindex_answer['answer'])
    
    # ç»¼åˆè¯„åˆ†
    print("\n" + "=" * 80)
    print("ğŸ† ç»¼åˆè¯„åˆ†")
    print("=" * 80)
    
    print("\nã€LangChainã€‘")
    print(f"  âœ… é€Ÿåº¦: {'å¿«' if speed_winner == 'LangChain' else 'æ…¢'}")
    print(f"  ğŸ“ å—å¤§å°: è¾ƒå° ({langchain_result['avg_chunk_size']:.0f} å­—ç¬¦)")
    print(f"  ğŸ“š ä¸Šä¸‹æ–‡å®Œæ•´æ€§: {'è¾ƒå·®' if langchain_result['avg_chunk_size'] < 1000 else 'è‰¯å¥½'}")
    print(f"  ğŸ¯ é€‚ç”¨åœºæ™¯: ç²¾ç¡®åŒ¹é…ã€çŸ­é—®é¢˜æ£€ç´¢")
    
    print("\nã€LlamaIndexã€‘")
    print(f"  âœ… é€Ÿåº¦: {'å¿«' if speed_winner == 'LlamaIndex' else 'æ…¢'}")
    print(f"  ğŸ“ å—å¤§å°: è¾ƒå¤§ ({llamaindex_result['avg_chunk_size']:.0f} å­—ç¬¦)")
    print(f"  ğŸ“š ä¸Šä¸‹æ–‡å®Œæ•´æ€§: {'è‰¯å¥½' if llamaindex_result['avg_chunk_size'] > 2000 else 'ä¸€èˆ¬'}")
    print(f"  ğŸ¯ é€‚ç”¨åœºæ™¯: å¤æ‚é—®é¢˜ã€éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡")
    
    print("\n" + "=" * 80)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    test_queries = [
        "æŠ—ç™Œè‚½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æŠ—ç™Œè‚½çš„ä¸»è¦ä½œç”¨æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•è®¾è®¡å’Œä¼˜åŒ–æŠ—ç™Œè‚½çš„ç»“æ„ï¼Ÿ",
    ]
    
    print("\n" + "=" * 80)
    print("ğŸ§ª LangChain vs LlamaIndex æ£€ç´¢æ•ˆæœå¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n\n{'='*80}")
        print(f"æµ‹è¯• {i}/{len(test_queries)}")
        print(f"{'='*80}")
        
        compare_retrieval_quality(query)
        
        if i < len(test_queries):
            input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")


if __name__ == "__main__":
    main()
